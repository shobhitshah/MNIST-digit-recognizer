{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-digit-recognizer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-eV_p1ZY32gN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialization\n"
      ]
    },
    {
      "metadata": {
        "id": "a5AQsn9532gR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn import ensemble\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U2pGg5kE32gf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reading part (1/n) of the data"
      ]
    },
    {
      "metadata": {
        "id": "lY8Sr_tM32gj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def skiprow(i, n):\n",
        "    if i % n == 0:\n",
        "       return False\n",
        "    return True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNIfF6jT32gr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Change n to load more, or less of the data."
      ]
    },
    {
      "metadata": {
        "id": "QwNrHiEy32gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b06ce663-95ff-4951-a843-284f456129ca"
      },
      "cell_type": "code",
      "source": [
        "n=5\n",
        "train_url='https://github.com/shobhitshah/MNIST-digit-recognizer/blob/develop/data/mnist-in-csv/mnist_train.csv.zip?raw=true'\n",
        "test_url='https://github.com/shobhitshah/MNIST-digit-recognizer/blob/develop/data/mnist-in-csv/mnist_test.csv.zip?raw=true'\n",
        "train = pd.read_csv(train_url, compression='zip', skiprows = lambda x: skiprow(x, n))\n",
        "test = pd.read_csv(test_url, compression='zip' , skiprows = lambda x: skiprow(x, 1))\n",
        "\n",
        "train.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 785)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "1NpkMPQC32g5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "35c2e0f3-0768-42ee-b953-a5ee6a0e665c"
      },
      "cell_type": "code",
      "source": [
        "train.sample(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>...</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10795</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8800</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5578</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9617</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8293</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...    28x19  \\\n",
              "10795      8    0    0    0    0    0    0    0    0    0  ...        0   \n",
              "8800       3    0    0    0    0    0    0    0    0    0  ...        0   \n",
              "5578       0    0    0    0    0    0    0    0    0    0  ...        0   \n",
              "9617       9    0    0    0    0    0    0    0    0    0  ...        0   \n",
              "8293       2    0    0    0    0    0    0    0    0    0  ...        0   \n",
              "\n",
              "       28x20  28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
              "10795      0      0      0      0      0      0      0      0      0  \n",
              "8800       0      0      0      0      0      0      0      0      0  \n",
              "5578       0      0      0      0      0      0      0      0      0  \n",
              "9617       0      0      0      0      0      0      0      0      0  \n",
              "8293       0      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "VoISWKSm32hA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "1d71eb2f-8165-4c56-fadd-802f205afc06"
      },
      "cell_type": "code",
      "source": [
        "df=train.drop('label', axis = 1)\n",
        "df.sample(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1x1</th>\n",
              "      <th>1x2</th>\n",
              "      <th>1x3</th>\n",
              "      <th>1x4</th>\n",
              "      <th>1x5</th>\n",
              "      <th>1x6</th>\n",
              "      <th>1x7</th>\n",
              "      <th>1x8</th>\n",
              "      <th>1x9</th>\n",
              "      <th>1x10</th>\n",
              "      <th>...</th>\n",
              "      <th>28x19</th>\n",
              "      <th>28x20</th>\n",
              "      <th>28x21</th>\n",
              "      <th>28x22</th>\n",
              "      <th>28x23</th>\n",
              "      <th>28x24</th>\n",
              "      <th>28x25</th>\n",
              "      <th>28x26</th>\n",
              "      <th>28x27</th>\n",
              "      <th>28x28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1859</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7320</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4612</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1853</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  1x10  ...    28x19  28x20  \\\n",
              "693     0    0    0    0    0    0    0    0    0     0  ...        0      0   \n",
              "1859    0    0    0    0    0    0    0    0    0     0  ...        0      0   \n",
              "7320    0    0    0    0    0    0    0    0    0     0  ...        0      0   \n",
              "4612    0    0    0    0    0    0    0    0    0     0  ...        0      0   \n",
              "1853    0    0    0    0    0    0    0    0    0     0  ...        0      0   \n",
              "\n",
              "      28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
              "693       0      0      0      0      0      0      0      0  \n",
              "1859      0      0      0      0      0      0      0      0  \n",
              "7320      0      0      0      0      0      0      0      0  \n",
              "4612      0      0      0      0      0      0      0      0  \n",
              "1853      0      0      0      0      0      0      0      0  \n",
              "\n",
              "[5 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "ytgQnyF132hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature selection"
      ]
    },
    {
      "metadata": {
        "id": "r0-ab7Ev32hG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y=train.label\n",
        "X=train.drop('label', axis=1)\n",
        "X_test = test.drop('label', axis=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G_rz-kbE32hJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define models"
      ]
    },
    {
      "metadata": {
        "id": "tLs1dh8B32hK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Multi label models."
      ]
    },
    {
      "metadata": {
        "id": "VbUxMwoi32hL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "models = []\n",
        "models.append(('KNN', KNeighborsClassifier(5)))\n",
        "models.append(('Linear SVM', SVC(gamma='scale', C=1, decision_function_shape='ovo', verbose=True)))\n",
        "models.append(('Neural Net', MLPClassifier(alpha=0.0001, solver='sgd', verbose=True)))\n",
        "models.append(('XGB', XGBClassifier(verbosity=2, objective='multi:softmax', num_class=10)))\n",
        "models.append(('LR', LogisticRegression(solver=\"saga\", multi_class=\"multinomial\", max_iter=1000, verbose=2))) # ovr - one versus reset\n",
        "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, verbose=2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JS1f3lbI32hN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model fitting and prediction"
      ]
    },
    {
      "metadata": {
        "id": "fjWX1HiC32hO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_score(name, model, X, y, val_y, y_pred, y_test, icv):\n",
        "    #print('Running CV score')\n",
        "    cvscore = cross_val_score(model, X, y, cv=icv)\n",
        "    print('Model {0} score: {1:.4f} and std dev: {2:.4f}'.format(name, cvscore.mean(), cvscore.std()))\n",
        "    print('Validation variance score: %.4f' % r2_score(val_y, y_pred))\n",
        "    print('Test variance score: %.4f' % r2_score(test.label, y_test))\n",
        "\n",
        "    return cvscore, r2_score(val_y, y_pred), r2_score(test.label, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IR2TfvGx32hR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cv_fit_and_predict(models, X, y):\n",
        "    results = []\n",
        "    train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
        "    for name, model in models:\n",
        "        #print(\"Fitting model {}\".format(name))\n",
        "        m = model.fit(train_X, train_y)\n",
        "        #print(\"Fitted model is {}\".format(m))\n",
        "        #print('Running prediction')\n",
        "        y_pred = model.predict(val_X)\n",
        "        y_test = model.predict(X_test)\n",
        "        score, val_r2, test_r2 = model_score(name, model, X, y, val_y, y_pred, y_test, 10)\n",
        "        results.append((name, score, val_r2, test_r2))\n",
        "    print(\"Done\")\n",
        "    return results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eNf7gcSb32hT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 30637
        },
        "outputId": "8cd57bfe-6ca8-4863-c1fb-fb1e14b340c6"
      },
      "cell_type": "code",
      "source": [
        "results = cv_fit_and_predict(models, X, y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model KNN score: 0.9441 and std dev: 0.0083\n",
            "Validation variance score: 0.8512\n",
            "Test variance score: 0.8851\n",
            "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]Model Linear SVM score: 0.9588 and std dev: 0.0059\n",
            "Validation variance score: 0.8990\n",
            "Test variance score: 0.9150\n",
            "Iteration 1, loss = 3.91112246\n",
            "Iteration 2, loss = 1.67939927\n",
            "Iteration 3, loss = 1.57028599\n",
            "Iteration 4, loss = 1.49816392\n",
            "Iteration 5, loss = 1.42333745\n",
            "Iteration 6, loss = 1.31552102\n",
            "Iteration 7, loss = 1.23247402\n",
            "Iteration 8, loss = 1.17491344\n",
            "Iteration 9, loss = 1.08049027\n",
            "Iteration 10, loss = 1.01417861\n",
            "Iteration 11, loss = 1.02138177\n",
            "Iteration 12, loss = 0.96542855\n",
            "Iteration 13, loss = 0.93406935\n",
            "Iteration 14, loss = 0.93911399\n",
            "Iteration 15, loss = 0.89218608\n",
            "Iteration 16, loss = 0.84963829\n",
            "Iteration 17, loss = 0.82408945\n",
            "Iteration 18, loss = 0.80728782\n",
            "Iteration 19, loss = 0.76045112\n",
            "Iteration 20, loss = 0.75054814\n",
            "Iteration 21, loss = 0.71704364\n",
            "Iteration 22, loss = 0.69445372\n",
            "Iteration 23, loss = 0.84811813\n",
            "Iteration 24, loss = 0.70943484\n",
            "Iteration 25, loss = 0.69556080\n",
            "Iteration 26, loss = 0.65310255\n",
            "Iteration 27, loss = 0.58726983\n",
            "Iteration 28, loss = 0.55957616\n",
            "Iteration 29, loss = 0.54908848\n",
            "Iteration 30, loss = 0.54100091\n",
            "Iteration 31, loss = 0.51210328\n",
            "Iteration 32, loss = 0.50687383\n",
            "Iteration 33, loss = 0.48742604\n",
            "Iteration 34, loss = 0.48218790\n",
            "Iteration 35, loss = 0.47844854\n",
            "Iteration 36, loss = 0.46876383\n",
            "Iteration 37, loss = 0.46588339\n",
            "Iteration 38, loss = 0.49657296\n",
            "Iteration 39, loss = 0.50649096\n",
            "Iteration 40, loss = 0.46676110\n",
            "Iteration 41, loss = 0.44273615\n",
            "Iteration 42, loss = 0.43063661\n",
            "Iteration 43, loss = 0.42468106\n",
            "Iteration 44, loss = 0.41267048\n",
            "Iteration 45, loss = 0.40477912\n",
            "Iteration 46, loss = 0.40101175\n",
            "Iteration 47, loss = 0.39902406\n",
            "Iteration 48, loss = 0.39034159\n",
            "Iteration 49, loss = 0.38564914\n",
            "Iteration 50, loss = 0.37739248\n",
            "Iteration 51, loss = 0.36704282\n",
            "Iteration 52, loss = 0.36133796\n",
            "Iteration 53, loss = 0.36081273\n",
            "Iteration 54, loss = 0.34989054\n",
            "Iteration 55, loss = 0.34525062\n",
            "Iteration 56, loss = 0.34847088\n",
            "Iteration 57, loss = 0.34279942\n",
            "Iteration 58, loss = 0.34115146\n",
            "Iteration 59, loss = 0.33286857\n",
            "Iteration 60, loss = 0.32968662\n",
            "Iteration 61, loss = 0.32866917\n",
            "Iteration 62, loss = 0.32683045\n",
            "Iteration 63, loss = 0.32218361\n",
            "Iteration 64, loss = 0.33770058\n",
            "Iteration 65, loss = 0.33051920\n",
            "Iteration 66, loss = 0.31224300\n",
            "Iteration 67, loss = 0.31349555\n",
            "Iteration 68, loss = 0.30447210\n",
            "Iteration 69, loss = 0.30181623\n",
            "Iteration 70, loss = 0.29984959\n",
            "Iteration 71, loss = 0.29526294\n",
            "Iteration 72, loss = 0.29573174\n",
            "Iteration 73, loss = 0.30626667\n",
            "Iteration 74, loss = 0.30426949\n",
            "Iteration 75, loss = 0.29374964\n",
            "Iteration 76, loss = 0.28689124\n",
            "Iteration 77, loss = 0.28505767\n",
            "Iteration 78, loss = 0.28667528\n",
            "Iteration 79, loss = 0.28064710\n",
            "Iteration 80, loss = 0.27843680\n",
            "Iteration 81, loss = 0.28017818\n",
            "Iteration 82, loss = 0.27297954\n",
            "Iteration 83, loss = 0.27270137\n",
            "Iteration 84, loss = 0.26820459\n",
            "Iteration 85, loss = 0.26731645\n",
            "Iteration 86, loss = 0.26963002\n",
            "Iteration 87, loss = 0.26419255\n",
            "Iteration 88, loss = 0.26572920\n",
            "Iteration 89, loss = 0.26512063\n",
            "Iteration 90, loss = 0.25843239\n",
            "Iteration 91, loss = 0.25549890\n",
            "Iteration 92, loss = 0.25488860\n",
            "Iteration 93, loss = 0.25948993\n",
            "Iteration 94, loss = 0.25209762\n",
            "Iteration 95, loss = 0.25500330\n",
            "Iteration 96, loss = 0.24966339\n",
            "Iteration 97, loss = 0.27557274\n",
            "Iteration 98, loss = 0.25890666\n",
            "Iteration 99, loss = 0.24568951\n",
            "Iteration 100, loss = 0.24577509\n",
            "Iteration 101, loss = 0.24007887\n",
            "Iteration 102, loss = 0.23948285\n",
            "Iteration 103, loss = 0.23766807\n",
            "Iteration 104, loss = 0.23386756\n",
            "Iteration 105, loss = 0.23344028\n",
            "Iteration 106, loss = 0.23345666\n",
            "Iteration 107, loss = 0.23313949\n",
            "Iteration 108, loss = 0.23121891\n",
            "Iteration 109, loss = 0.23776579\n",
            "Iteration 110, loss = 0.23248908\n",
            "Iteration 111, loss = 0.22973791\n",
            "Iteration 112, loss = 0.22784586\n",
            "Iteration 113, loss = 0.22630162\n",
            "Iteration 114, loss = 0.22625676\n",
            "Iteration 115, loss = 0.22718339\n",
            "Iteration 116, loss = 0.22450557\n",
            "Iteration 117, loss = 0.22196959\n",
            "Iteration 118, loss = 0.22428671\n",
            "Iteration 119, loss = 0.22735978\n",
            "Iteration 120, loss = 0.22376869\n",
            "Iteration 121, loss = 0.22417251\n",
            "Iteration 122, loss = 0.21968978\n",
            "Iteration 123, loss = 0.21689667\n",
            "Iteration 124, loss = 0.21790690\n",
            "Iteration 125, loss = 0.21798622\n",
            "Iteration 126, loss = 0.21585735\n",
            "Iteration 127, loss = 0.21554673\n",
            "Iteration 128, loss = 0.21429796\n",
            "Iteration 129, loss = 0.21685091\n",
            "Iteration 130, loss = 0.21097302\n",
            "Iteration 131, loss = 0.21257561\n",
            "Iteration 132, loss = 0.21214946\n",
            "Iteration 133, loss = 0.21489100\n",
            "Iteration 134, loss = 0.21240017\n",
            "Iteration 135, loss = 0.21281302\n",
            "Iteration 136, loss = 0.21341288\n",
            "Iteration 137, loss = 0.21115328\n",
            "Iteration 138, loss = 0.21061936\n",
            "Iteration 139, loss = 0.20980329\n",
            "Iteration 140, loss = 0.21661807\n",
            "Iteration 141, loss = 0.20874057\n",
            "Iteration 142, loss = 0.20519111\n",
            "Iteration 143, loss = 0.20884490\n",
            "Iteration 144, loss = 0.20519071\n",
            "Iteration 145, loss = 0.20260562\n",
            "Iteration 146, loss = 0.20245620\n",
            "Iteration 147, loss = 0.20120394\n",
            "Iteration 148, loss = 0.20187939\n",
            "Iteration 149, loss = 0.20144128\n",
            "Iteration 150, loss = 0.20601954\n",
            "Iteration 151, loss = 0.20674003\n",
            "Iteration 152, loss = 0.20396392\n",
            "Iteration 153, loss = 0.20523223\n",
            "Iteration 154, loss = 0.20211300\n",
            "Iteration 155, loss = 0.20176830\n",
            "Iteration 156, loss = 0.20246491\n",
            "Iteration 157, loss = 0.19866764\n",
            "Iteration 158, loss = 0.19892628\n",
            "Iteration 159, loss = 0.19840372\n",
            "Iteration 160, loss = 0.19812107\n",
            "Iteration 161, loss = 0.19621246\n",
            "Iteration 162, loss = 0.20581962\n",
            "Iteration 163, loss = 0.19857488\n",
            "Iteration 164, loss = 0.20264362\n",
            "Iteration 165, loss = 0.20099759\n",
            "Iteration 166, loss = 0.19629596\n",
            "Iteration 167, loss = 0.19357196\n",
            "Iteration 168, loss = 0.19215106\n",
            "Iteration 169, loss = 0.19992276\n",
            "Iteration 170, loss = 0.19707587\n",
            "Iteration 171, loss = 0.19306497\n",
            "Iteration 172, loss = 0.19587146\n",
            "Iteration 173, loss = 0.19320074\n",
            "Iteration 174, loss = 0.19787962\n",
            "Iteration 175, loss = 0.19820808\n",
            "Iteration 176, loss = 0.20152445\n",
            "Iteration 177, loss = 0.19803984\n",
            "Iteration 178, loss = 0.19319848\n",
            "Iteration 179, loss = 0.19042071\n",
            "Iteration 180, loss = 0.19038483\n",
            "Iteration 181, loss = 0.18903746\n",
            "Iteration 182, loss = 0.19027209\n",
            "Iteration 183, loss = 0.18731302\n",
            "Iteration 184, loss = 0.18996930\n",
            "Iteration 185, loss = 0.18865848\n",
            "Iteration 186, loss = 0.19325481\n",
            "Iteration 187, loss = 0.20247930\n",
            "Iteration 188, loss = 0.19106477\n",
            "Iteration 189, loss = 0.19256457\n",
            "Iteration 190, loss = 0.18995219\n",
            "Iteration 191, loss = 0.19252009\n",
            "Iteration 192, loss = 0.18909964\n",
            "Iteration 193, loss = 0.18908602\n",
            "Iteration 194, loss = 0.19004990\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.55664169\n",
            "Iteration 2, loss = 1.51248835\n",
            "Iteration 3, loss = 1.34893256\n",
            "Iteration 4, loss = 1.15090495\n",
            "Iteration 5, loss = 1.08954212\n",
            "Iteration 6, loss = 0.97484317\n",
            "Iteration 7, loss = 0.87668432\n",
            "Iteration 8, loss = 0.82103558\n",
            "Iteration 9, loss = 0.75825832\n",
            "Iteration 10, loss = 0.71100560\n",
            "Iteration 11, loss = 0.66367266\n",
            "Iteration 12, loss = 0.63283085\n",
            "Iteration 13, loss = 0.60480463\n",
            "Iteration 14, loss = 0.58201814\n",
            "Iteration 15, loss = 0.56499801\n",
            "Iteration 16, loss = 0.54432475\n",
            "Iteration 17, loss = 0.53164280\n",
            "Iteration 18, loss = 0.51957623\n",
            "Iteration 19, loss = 0.50382612\n",
            "Iteration 20, loss = 0.48725870\n",
            "Iteration 21, loss = 0.47844975\n",
            "Iteration 22, loss = 0.47016317\n",
            "Iteration 23, loss = 0.46199131\n",
            "Iteration 24, loss = 0.45940691\n",
            "Iteration 25, loss = 0.44920885\n",
            "Iteration 26, loss = 0.44042751\n",
            "Iteration 27, loss = 0.43311266\n",
            "Iteration 28, loss = 0.42773504\n",
            "Iteration 29, loss = 0.41914659\n",
            "Iteration 30, loss = 0.41723909\n",
            "Iteration 31, loss = 0.40874513\n",
            "Iteration 32, loss = 0.39922085\n",
            "Iteration 33, loss = 0.39510653\n",
            "Iteration 34, loss = 0.38947644\n",
            "Iteration 35, loss = 0.38486578\n",
            "Iteration 36, loss = 0.37770804\n",
            "Iteration 37, loss = 0.37546789\n",
            "Iteration 38, loss = 0.37139940\n",
            "Iteration 39, loss = 0.36365699\n",
            "Iteration 40, loss = 0.36618527\n",
            "Iteration 41, loss = 0.35713721\n",
            "Iteration 42, loss = 0.34984596\n",
            "Iteration 43, loss = 0.34315899\n",
            "Iteration 44, loss = 0.34321342\n",
            "Iteration 45, loss = 0.33739973\n",
            "Iteration 46, loss = 0.33537369\n",
            "Iteration 47, loss = 0.33207317\n",
            "Iteration 48, loss = 0.32297243\n",
            "Iteration 49, loss = 0.31709995\n",
            "Iteration 50, loss = 0.31900983\n",
            "Iteration 51, loss = 0.31596456\n",
            "Iteration 52, loss = 0.31060487\n",
            "Iteration 53, loss = 0.30977222\n",
            "Iteration 54, loss = 0.30369767\n",
            "Iteration 55, loss = 0.30111139\n",
            "Iteration 56, loss = 0.29657632\n",
            "Iteration 57, loss = 0.29733244\n",
            "Iteration 58, loss = 0.29164050\n",
            "Iteration 59, loss = 0.29354552\n",
            "Iteration 60, loss = 0.28935571\n",
            "Iteration 61, loss = 0.28549205\n",
            "Iteration 62, loss = 0.28785684\n",
            "Iteration 63, loss = 0.28488288\n",
            "Iteration 64, loss = 0.28461779\n",
            "Iteration 65, loss = 0.28221614\n",
            "Iteration 66, loss = 0.27779495\n",
            "Iteration 67, loss = 0.27657205\n",
            "Iteration 68, loss = 0.27412921\n",
            "Iteration 69, loss = 0.27332176\n",
            "Iteration 70, loss = 0.27363404\n",
            "Iteration 71, loss = 0.26402734\n",
            "Iteration 72, loss = 0.26549832\n",
            "Iteration 73, loss = 0.26334165\n",
            "Iteration 74, loss = 0.25999485\n",
            "Iteration 75, loss = 0.26192713\n",
            "Iteration 76, loss = 0.26162602\n",
            "Iteration 77, loss = 0.25932069\n",
            "Iteration 78, loss = 0.25680511\n",
            "Iteration 79, loss = 0.25278281\n",
            "Iteration 80, loss = 0.25057480\n",
            "Iteration 81, loss = 0.25677535\n",
            "Iteration 82, loss = 0.25478465\n",
            "Iteration 83, loss = 0.25144412\n",
            "Iteration 84, loss = 0.24781521\n",
            "Iteration 85, loss = 0.24887644\n",
            "Iteration 86, loss = 0.24506767\n",
            "Iteration 87, loss = 0.24490310\n",
            "Iteration 88, loss = 0.24287724\n",
            "Iteration 89, loss = 0.24689766\n",
            "Iteration 90, loss = 0.24418508\n",
            "Iteration 91, loss = 0.24077225\n",
            "Iteration 92, loss = 0.24416753\n",
            "Iteration 93, loss = 0.23947649\n",
            "Iteration 94, loss = 0.23774130\n",
            "Iteration 95, loss = 0.23819769\n",
            "Iteration 96, loss = 0.23719132\n",
            "Iteration 97, loss = 0.23404734\n",
            "Iteration 98, loss = 0.23684923\n",
            "Iteration 99, loss = 0.23682538\n",
            "Iteration 100, loss = 0.23911764\n",
            "Iteration 101, loss = 0.23860159\n",
            "Iteration 102, loss = 0.23177728\n",
            "Iteration 103, loss = 0.23256472\n",
            "Iteration 104, loss = 0.23092428\n",
            "Iteration 105, loss = 0.23085575\n",
            "Iteration 106, loss = 0.22876836\n",
            "Iteration 107, loss = 0.23012392\n",
            "Iteration 108, loss = 0.22500140\n",
            "Iteration 109, loss = 0.22276659\n",
            "Iteration 110, loss = 0.22393946\n",
            "Iteration 111, loss = 0.22080224\n",
            "Iteration 112, loss = 0.22003599\n",
            "Iteration 113, loss = 0.22058038\n",
            "Iteration 114, loss = 0.21812012\n",
            "Iteration 115, loss = 0.21892111\n",
            "Iteration 116, loss = 0.21829194\n",
            "Iteration 117, loss = 0.22032001\n",
            "Iteration 118, loss = 0.21676518\n",
            "Iteration 119, loss = 0.21616602\n",
            "Iteration 120, loss = 0.21474547\n",
            "Iteration 121, loss = 0.21435399\n",
            "Iteration 122, loss = 0.21500855\n",
            "Iteration 123, loss = 0.21380003\n",
            "Iteration 124, loss = 0.21482492\n",
            "Iteration 125, loss = 0.21711294\n",
            "Iteration 126, loss = 0.21128964\n",
            "Iteration 127, loss = 0.21846806\n",
            "Iteration 128, loss = 0.21465429\n",
            "Iteration 129, loss = 0.21279981\n",
            "Iteration 130, loss = 0.20919109\n",
            "Iteration 131, loss = 0.21000371\n",
            "Iteration 132, loss = 0.21234046\n",
            "Iteration 133, loss = 0.20944357\n",
            "Iteration 134, loss = 0.20806679\n",
            "Iteration 135, loss = 0.20926994\n",
            "Iteration 136, loss = 0.21111469\n",
            "Iteration 137, loss = 0.20906526\n",
            "Iteration 138, loss = 0.21388237\n",
            "Iteration 139, loss = 0.21061874\n",
            "Iteration 140, loss = 0.21116914\n",
            "Iteration 141, loss = 0.20933729\n",
            "Iteration 142, loss = 0.20686818\n",
            "Iteration 143, loss = 0.20719683\n",
            "Iteration 144, loss = 0.20713631\n",
            "Iteration 145, loss = 0.20632502\n",
            "Iteration 146, loss = 0.20497605\n",
            "Iteration 147, loss = 0.20741984\n",
            "Iteration 148, loss = 0.20442233\n",
            "Iteration 149, loss = 0.20492294\n",
            "Iteration 150, loss = 0.20544725\n",
            "Iteration 151, loss = 0.20432529\n",
            "Iteration 152, loss = 0.20091045\n",
            "Iteration 153, loss = 0.20212796\n",
            "Iteration 154, loss = 0.20189934\n",
            "Iteration 155, loss = 0.20507020\n",
            "Iteration 156, loss = 0.20389177\n",
            "Iteration 157, loss = 0.20056114\n",
            "Iteration 158, loss = 0.19811064\n",
            "Iteration 159, loss = 0.19986333\n",
            "Iteration 160, loss = 0.20031094\n",
            "Iteration 161, loss = 0.20223286\n",
            "Iteration 162, loss = 0.20016715\n",
            "Iteration 163, loss = 0.20060575\n",
            "Iteration 164, loss = 0.19988011\n",
            "Iteration 165, loss = 0.20031618\n",
            "Iteration 166, loss = 0.19805089\n",
            "Iteration 167, loss = 0.19786337\n",
            "Iteration 168, loss = 0.20003154\n",
            "Iteration 169, loss = 0.20132685\n",
            "Iteration 170, loss = 0.19718476\n",
            "Iteration 171, loss = 0.20004818\n",
            "Iteration 172, loss = 0.19696063\n",
            "Iteration 173, loss = 0.19304434\n",
            "Iteration 174, loss = 0.19118689\n",
            "Iteration 175, loss = 0.18918638\n",
            "Iteration 176, loss = 0.19006182\n",
            "Iteration 177, loss = 0.19017527\n",
            "Iteration 178, loss = 0.20018986\n",
            "Iteration 179, loss = 0.19286853\n",
            "Iteration 180, loss = 0.20060913\n",
            "Iteration 181, loss = 0.19137493\n",
            "Iteration 182, loss = 0.19223632\n",
            "Iteration 183, loss = 0.19082633\n",
            "Iteration 184, loss = 0.18779510\n",
            "Iteration 185, loss = 0.18664628\n",
            "Iteration 186, loss = 0.19268353\n",
            "Iteration 187, loss = 0.19230019\n",
            "Iteration 188, loss = 0.18714095\n",
            "Iteration 189, loss = 0.18369913\n",
            "Iteration 190, loss = 0.18275799\n",
            "Iteration 191, loss = 0.18217289\n",
            "Iteration 192, loss = 0.18299672\n",
            "Iteration 193, loss = 0.18096746\n",
            "Iteration 194, loss = 0.18212234\n",
            "Iteration 195, loss = 0.18552217\n",
            "Iteration 196, loss = 0.18185928\n",
            "Iteration 197, loss = 0.18750807\n",
            "Iteration 198, loss = 0.18447569\n",
            "Iteration 199, loss = 0.18777541\n",
            "Iteration 200, loss = 0.18528792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.51180918\n",
            "Iteration 2, loss = 1.44530864\n",
            "Iteration 3, loss = 1.16270343\n",
            "Iteration 4, loss = 1.04757208\n",
            "Iteration 5, loss = 0.93894623\n",
            "Iteration 6, loss = 0.87305515\n",
            "Iteration 7, loss = 0.81354910\n",
            "Iteration 8, loss = 0.76717038\n",
            "Iteration 9, loss = 0.69548090\n",
            "Iteration 10, loss = 0.64070735\n",
            "Iteration 11, loss = 0.61200155\n",
            "Iteration 12, loss = 0.58691702\n",
            "Iteration 13, loss = 0.56926865\n",
            "Iteration 14, loss = 0.56218429\n",
            "Iteration 15, loss = 0.54164667\n",
            "Iteration 16, loss = 0.51804031\n",
            "Iteration 17, loss = 0.53206126\n",
            "Iteration 18, loss = 0.49727502\n",
            "Iteration 19, loss = 0.47937919\n",
            "Iteration 20, loss = 0.47159766\n",
            "Iteration 21, loss = 0.46802905\n",
            "Iteration 22, loss = 0.44976282\n",
            "Iteration 23, loss = 0.42949420\n",
            "Iteration 24, loss = 0.42609046\n",
            "Iteration 25, loss = 0.42290442\n",
            "Iteration 26, loss = 0.41082415\n",
            "Iteration 27, loss = 0.40453186\n",
            "Iteration 28, loss = 0.39740299\n",
            "Iteration 29, loss = 0.39378269\n",
            "Iteration 30, loss = 0.38652492\n",
            "Iteration 31, loss = 0.37800965\n",
            "Iteration 32, loss = 0.37465340\n",
            "Iteration 33, loss = 0.37021743\n",
            "Iteration 34, loss = 0.36606809\n",
            "Iteration 35, loss = 0.35205597\n",
            "Iteration 36, loss = 0.35805267\n",
            "Iteration 37, loss = 0.34826114\n",
            "Iteration 38, loss = 0.34105711\n",
            "Iteration 39, loss = 0.33895137\n",
            "Iteration 40, loss = 0.33675833\n",
            "Iteration 41, loss = 0.33356045\n",
            "Iteration 42, loss = 0.32816646\n",
            "Iteration 43, loss = 0.32859693\n",
            "Iteration 44, loss = 0.32466025\n",
            "Iteration 45, loss = 0.31813123\n",
            "Iteration 46, loss = 0.31468647\n",
            "Iteration 47, loss = 0.31642005\n",
            "Iteration 48, loss = 0.31303392\n",
            "Iteration 49, loss = 0.30750052\n",
            "Iteration 50, loss = 0.30236392\n",
            "Iteration 51, loss = 0.30382069\n",
            "Iteration 52, loss = 0.29948981\n",
            "Iteration 53, loss = 0.29699564\n",
            "Iteration 54, loss = 0.29877186\n",
            "Iteration 55, loss = 0.29083407\n",
            "Iteration 56, loss = 0.29313381\n",
            "Iteration 57, loss = 0.29015770\n",
            "Iteration 58, loss = 0.28260506\n",
            "Iteration 59, loss = 0.29593105\n",
            "Iteration 60, loss = 0.28117955\n",
            "Iteration 61, loss = 0.27879484\n",
            "Iteration 62, loss = 0.27694802\n",
            "Iteration 63, loss = 0.27514557\n",
            "Iteration 64, loss = 0.27245234\n",
            "Iteration 65, loss = 0.27164212\n",
            "Iteration 66, loss = 0.27134590\n",
            "Iteration 67, loss = 0.26729344\n",
            "Iteration 68, loss = 0.26593125\n",
            "Iteration 69, loss = 0.26443449\n",
            "Iteration 70, loss = 0.26319007\n",
            "Iteration 71, loss = 0.25997273\n",
            "Iteration 72, loss = 0.25215777\n",
            "Iteration 73, loss = 0.25037700\n",
            "Iteration 74, loss = 0.25414314\n",
            "Iteration 75, loss = 0.24621298\n",
            "Iteration 76, loss = 0.24686121\n",
            "Iteration 77, loss = 0.24435383\n",
            "Iteration 78, loss = 0.24220957\n",
            "Iteration 79, loss = 0.24381981\n",
            "Iteration 80, loss = 0.23922505\n",
            "Iteration 81, loss = 0.24232680\n",
            "Iteration 82, loss = 0.23835611\n",
            "Iteration 83, loss = 0.23865784\n",
            "Iteration 84, loss = 0.23666496\n",
            "Iteration 85, loss = 0.23924166\n",
            "Iteration 86, loss = 0.23584833\n",
            "Iteration 87, loss = 0.23146077\n",
            "Iteration 88, loss = 0.23156646\n",
            "Iteration 89, loss = 0.22846010\n",
            "Iteration 90, loss = 0.22859999\n",
            "Iteration 91, loss = 0.22911401\n",
            "Iteration 92, loss = 0.22558215\n",
            "Iteration 93, loss = 0.22516818\n",
            "Iteration 94, loss = 0.22290589\n",
            "Iteration 95, loss = 0.21937879\n",
            "Iteration 96, loss = 0.21847935\n",
            "Iteration 97, loss = 0.21847431\n",
            "Iteration 98, loss = 0.21552362\n",
            "Iteration 99, loss = 0.21555171\n",
            "Iteration 100, loss = 0.21667133\n",
            "Iteration 101, loss = 0.21416348\n",
            "Iteration 102, loss = 0.21614571\n",
            "Iteration 103, loss = 0.21415800\n",
            "Iteration 104, loss = 0.20825310\n",
            "Iteration 105, loss = 0.20735204\n",
            "Iteration 106, loss = 0.21150500\n",
            "Iteration 107, loss = 0.20701514\n",
            "Iteration 108, loss = 0.21380705\n",
            "Iteration 109, loss = 0.20771614\n",
            "Iteration 110, loss = 0.20952906\n",
            "Iteration 111, loss = 0.20427397\n",
            "Iteration 112, loss = 0.20579156\n",
            "Iteration 113, loss = 0.20256617\n",
            "Iteration 114, loss = 0.20083751\n",
            "Iteration 115, loss = 0.20122942\n",
            "Iteration 116, loss = 0.20229839\n",
            "Iteration 117, loss = 0.19974700\n",
            "Iteration 118, loss = 0.19621263\n",
            "Iteration 119, loss = 0.20081958\n",
            "Iteration 120, loss = 0.19987024\n",
            "Iteration 121, loss = 0.19920293\n",
            "Iteration 122, loss = 0.19547920\n",
            "Iteration 123, loss = 0.19354772\n",
            "Iteration 124, loss = 0.19208918\n",
            "Iteration 125, loss = 0.19958395\n",
            "Iteration 126, loss = 0.19906559\n",
            "Iteration 127, loss = 0.19513734\n",
            "Iteration 128, loss = 0.19042292\n",
            "Iteration 129, loss = 0.19226860\n",
            "Iteration 130, loss = 0.19057105\n",
            "Iteration 131, loss = 0.18769387\n",
            "Iteration 132, loss = 0.19324099\n",
            "Iteration 133, loss = 0.19047958\n",
            "Iteration 134, loss = 0.18934753\n",
            "Iteration 135, loss = 0.18921768\n",
            "Iteration 136, loss = 0.18806863\n",
            "Iteration 137, loss = 0.18802166\n",
            "Iteration 138, loss = 0.18398770\n",
            "Iteration 139, loss = 0.18155414\n",
            "Iteration 140, loss = 0.18516046\n",
            "Iteration 141, loss = 0.18280327\n",
            "Iteration 142, loss = 0.18735540\n",
            "Iteration 143, loss = 0.18715432\n",
            "Iteration 144, loss = 0.18100530\n",
            "Iteration 145, loss = 0.18167609\n",
            "Iteration 146, loss = 0.18292325\n",
            "Iteration 147, loss = 0.18051338\n",
            "Iteration 148, loss = 0.17523681\n",
            "Iteration 149, loss = 0.18159702\n",
            "Iteration 150, loss = 0.17666188\n",
            "Iteration 151, loss = 0.18281856\n",
            "Iteration 152, loss = 0.17621819\n",
            "Iteration 153, loss = 0.17567122\n",
            "Iteration 154, loss = 0.17225019\n",
            "Iteration 155, loss = 0.17239105\n",
            "Iteration 156, loss = 0.17056913\n",
            "Iteration 157, loss = 0.16627216\n",
            "Iteration 158, loss = 0.16722647\n",
            "Iteration 159, loss = 0.17226551\n",
            "Iteration 160, loss = 0.17068468\n",
            "Iteration 161, loss = 0.16640048\n",
            "Iteration 162, loss = 0.16539659\n",
            "Iteration 163, loss = 0.16568353\n",
            "Iteration 164, loss = 0.16442730\n",
            "Iteration 165, loss = 0.15939267\n",
            "Iteration 166, loss = 0.16677945\n",
            "Iteration 167, loss = 0.17416444\n",
            "Iteration 168, loss = 0.16709877\n",
            "Iteration 169, loss = 0.16018033\n",
            "Iteration 170, loss = 0.16329211\n",
            "Iteration 171, loss = 0.16785840\n",
            "Iteration 172, loss = 0.16470954\n",
            "Iteration 173, loss = 0.16152003\n",
            "Iteration 174, loss = 0.16075432\n",
            "Iteration 175, loss = 0.15869100\n",
            "Iteration 176, loss = 0.15939506\n",
            "Iteration 177, loss = 0.16104302\n",
            "Iteration 178, loss = 0.16257313\n",
            "Iteration 179, loss = 0.16539212\n",
            "Iteration 180, loss = 0.15789688\n",
            "Iteration 181, loss = 0.15692625\n",
            "Iteration 182, loss = 0.15822620\n",
            "Iteration 183, loss = 0.15530862\n",
            "Iteration 184, loss = 0.15605995\n",
            "Iteration 185, loss = 0.15762195\n",
            "Iteration 186, loss = 0.16125215\n",
            "Iteration 187, loss = 0.15970002\n",
            "Iteration 188, loss = 0.16574281\n",
            "Iteration 189, loss = 0.16136383\n",
            "Iteration 190, loss = 0.15512612\n",
            "Iteration 191, loss = 0.15604487\n",
            "Iteration 192, loss = 0.16146026\n",
            "Iteration 193, loss = 0.17286234\n",
            "Iteration 194, loss = 0.16791625\n",
            "Iteration 195, loss = 0.15704413\n",
            "Iteration 196, loss = 0.15491053\n",
            "Iteration 197, loss = 0.15524032\n",
            "Iteration 198, loss = 0.15394422\n",
            "Iteration 199, loss = 0.15794568\n",
            "Iteration 200, loss = 0.15560000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.52383630\n",
            "Iteration 2, loss = 1.55754605\n",
            "Iteration 3, loss = 1.32164956\n",
            "Iteration 4, loss = 1.19490324\n",
            "Iteration 5, loss = 1.06590965\n",
            "Iteration 6, loss = 0.94531834\n",
            "Iteration 7, loss = 0.89967101\n",
            "Iteration 8, loss = 0.79628305\n",
            "Iteration 9, loss = 0.73599057\n",
            "Iteration 10, loss = 0.72151953\n",
            "Iteration 11, loss = 0.66884856\n",
            "Iteration 12, loss = 0.61309090\n",
            "Iteration 13, loss = 0.56298360\n",
            "Iteration 14, loss = 0.53363632\n",
            "Iteration 15, loss = 0.52126916\n",
            "Iteration 16, loss = 0.48496822\n",
            "Iteration 17, loss = 0.45416203\n",
            "Iteration 18, loss = 0.43277741\n",
            "Iteration 19, loss = 0.41822342\n",
            "Iteration 20, loss = 0.39983471\n",
            "Iteration 21, loss = 0.38402234\n",
            "Iteration 22, loss = 0.37189193\n",
            "Iteration 23, loss = 0.36082464\n",
            "Iteration 24, loss = 0.35455858\n",
            "Iteration 25, loss = 0.34954012\n",
            "Iteration 26, loss = 0.34209095\n",
            "Iteration 27, loss = 0.33113990\n",
            "Iteration 28, loss = 0.32850039\n",
            "Iteration 29, loss = 0.33625692\n",
            "Iteration 30, loss = 0.31603686\n",
            "Iteration 31, loss = 0.31259436\n",
            "Iteration 32, loss = 0.30456515\n",
            "Iteration 33, loss = 0.29782300\n",
            "Iteration 34, loss = 0.28999167\n",
            "Iteration 35, loss = 0.28781505\n",
            "Iteration 36, loss = 0.28436750\n",
            "Iteration 37, loss = 0.27799085\n",
            "Iteration 38, loss = 0.26863997\n",
            "Iteration 39, loss = 0.26827009\n",
            "Iteration 40, loss = 0.26881930\n",
            "Iteration 41, loss = 0.26102700\n",
            "Iteration 42, loss = 0.25973022\n",
            "Iteration 43, loss = 0.25996188\n",
            "Iteration 44, loss = 0.25670049\n",
            "Iteration 45, loss = 0.25378668\n",
            "Iteration 46, loss = 0.25546192\n",
            "Iteration 47, loss = 0.24715402\n",
            "Iteration 48, loss = 0.24550248\n",
            "Iteration 49, loss = 0.24420773\n",
            "Iteration 50, loss = 0.23992653\n",
            "Iteration 51, loss = 0.23631920\n",
            "Iteration 52, loss = 0.24120862\n",
            "Iteration 53, loss = 0.23124148\n",
            "Iteration 54, loss = 0.22963565\n",
            "Iteration 55, loss = 0.22608212\n",
            "Iteration 56, loss = 0.22735591\n",
            "Iteration 57, loss = 0.23053781\n",
            "Iteration 58, loss = 0.22200220\n",
            "Iteration 59, loss = 0.22792729\n",
            "Iteration 60, loss = 0.22048213\n",
            "Iteration 61, loss = 0.22102972\n",
            "Iteration 62, loss = 0.21707695\n",
            "Iteration 63, loss = 0.21412217\n",
            "Iteration 64, loss = 0.22086969\n",
            "Iteration 65, loss = 0.21289667\n",
            "Iteration 66, loss = 0.20873841\n",
            "Iteration 67, loss = 0.20696149\n",
            "Iteration 68, loss = 0.21270830\n",
            "Iteration 69, loss = 0.20708417\n",
            "Iteration 70, loss = 0.20493851\n",
            "Iteration 71, loss = 0.20849774\n",
            "Iteration 72, loss = 0.21211988\n",
            "Iteration 73, loss = 0.20042348\n",
            "Iteration 74, loss = 0.19999328\n",
            "Iteration 75, loss = 0.19809009\n",
            "Iteration 76, loss = 0.20124401\n",
            "Iteration 77, loss = 0.19687462\n",
            "Iteration 78, loss = 0.19780824\n",
            "Iteration 79, loss = 0.19566236\n",
            "Iteration 80, loss = 0.19588697\n",
            "Iteration 81, loss = 0.19097044\n",
            "Iteration 82, loss = 0.19411239\n",
            "Iteration 83, loss = 0.18586334\n",
            "Iteration 84, loss = 0.19009312\n",
            "Iteration 85, loss = 0.18782999\n",
            "Iteration 86, loss = 0.18617972\n",
            "Iteration 87, loss = 0.18452034\n",
            "Iteration 88, loss = 0.18094269\n",
            "Iteration 89, loss = 0.18781349\n",
            "Iteration 90, loss = 0.18168927\n",
            "Iteration 91, loss = 0.18317662\n",
            "Iteration 92, loss = 0.17925704\n",
            "Iteration 93, loss = 0.18109615\n",
            "Iteration 94, loss = 0.18250252\n",
            "Iteration 95, loss = 0.17706368\n",
            "Iteration 96, loss = 0.17768209\n",
            "Iteration 97, loss = 0.17803009\n",
            "Iteration 98, loss = 0.17205044\n",
            "Iteration 99, loss = 0.17215641\n",
            "Iteration 100, loss = 0.16803622\n",
            "Iteration 101, loss = 0.16919078\n",
            "Iteration 102, loss = 0.16892302\n",
            "Iteration 103, loss = 0.16811285\n",
            "Iteration 104, loss = 0.16734510\n",
            "Iteration 105, loss = 0.16640723\n",
            "Iteration 106, loss = 0.16536714\n",
            "Iteration 107, loss = 0.16314114\n",
            "Iteration 108, loss = 0.17204241\n",
            "Iteration 109, loss = 0.16903502\n",
            "Iteration 110, loss = 0.17053938\n",
            "Iteration 111, loss = 0.16358055\n",
            "Iteration 112, loss = 0.16907509\n",
            "Iteration 113, loss = 0.16356434\n",
            "Iteration 114, loss = 0.15863283\n",
            "Iteration 115, loss = 0.16192678\n",
            "Iteration 116, loss = 0.15981225\n",
            "Iteration 117, loss = 0.16307891\n",
            "Iteration 118, loss = 0.15948968\n",
            "Iteration 119, loss = 0.15915954\n",
            "Iteration 120, loss = 0.15923396\n",
            "Iteration 121, loss = 0.15172222\n",
            "Iteration 122, loss = 0.15730983\n",
            "Iteration 123, loss = 0.15687224\n",
            "Iteration 124, loss = 0.16081320\n",
            "Iteration 125, loss = 0.14929015\n",
            "Iteration 126, loss = 0.15546161\n",
            "Iteration 127, loss = 0.15128650\n",
            "Iteration 128, loss = 0.14930446\n",
            "Iteration 129, loss = 0.14750971\n",
            "Iteration 130, loss = 0.14802014\n",
            "Iteration 131, loss = 0.14884122\n",
            "Iteration 132, loss = 0.15239338\n",
            "Iteration 133, loss = 0.15020011\n",
            "Iteration 134, loss = 0.15268739\n",
            "Iteration 135, loss = 0.15251883\n",
            "Iteration 136, loss = 0.14865194\n",
            "Iteration 137, loss = 0.15218904\n",
            "Iteration 138, loss = 0.15017176\n",
            "Iteration 139, loss = 0.14925841\n",
            "Iteration 140, loss = 0.14704957\n",
            "Iteration 141, loss = 0.15131158\n",
            "Iteration 142, loss = 0.14974114\n",
            "Iteration 143, loss = 0.14657139\n",
            "Iteration 144, loss = 0.15209537\n",
            "Iteration 145, loss = 0.15511663\n",
            "Iteration 146, loss = 0.14970787\n",
            "Iteration 147, loss = 0.15603791\n",
            "Iteration 148, loss = 0.14514174\n",
            "Iteration 149, loss = 0.14558902\n",
            "Iteration 150, loss = 0.14431577\n",
            "Iteration 151, loss = 0.14435218\n",
            "Iteration 152, loss = 0.13986063\n",
            "Iteration 153, loss = 0.13642565\n",
            "Iteration 154, loss = 0.14353207\n",
            "Iteration 155, loss = 0.14630131\n",
            "Iteration 156, loss = 0.13848419\n",
            "Iteration 157, loss = 0.14350966\n",
            "Iteration 158, loss = 0.14427489\n",
            "Iteration 159, loss = 0.14254638\n",
            "Iteration 160, loss = 0.14352596\n",
            "Iteration 161, loss = 0.13913806\n",
            "Iteration 162, loss = 0.13764658\n",
            "Iteration 163, loss = 0.13440079\n",
            "Iteration 164, loss = 0.14125131\n",
            "Iteration 165, loss = 0.14029819\n",
            "Iteration 166, loss = 0.14039810\n",
            "Iteration 167, loss = 0.13783721\n",
            "Iteration 168, loss = 0.13508219\n",
            "Iteration 169, loss = 0.14412846\n",
            "Iteration 170, loss = 0.14662040\n",
            "Iteration 171, loss = 0.14243759\n",
            "Iteration 172, loss = 0.13779010\n",
            "Iteration 173, loss = 0.13361849\n",
            "Iteration 174, loss = 0.12841070\n",
            "Iteration 175, loss = 0.14345856\n",
            "Iteration 176, loss = 0.14267080\n",
            "Iteration 177, loss = 0.13584468\n",
            "Iteration 178, loss = 0.13689371\n",
            "Iteration 179, loss = 0.13483613\n",
            "Iteration 180, loss = 0.13770265\n",
            "Iteration 181, loss = 0.13277181\n",
            "Iteration 182, loss = 0.13812877\n",
            "Iteration 183, loss = 0.13535914\n",
            "Iteration 184, loss = 0.13614122\n",
            "Iteration 185, loss = 0.12705261\n",
            "Iteration 186, loss = 0.13514107\n",
            "Iteration 187, loss = 0.12712386\n",
            "Iteration 188, loss = 0.12653436\n",
            "Iteration 189, loss = 0.12927017\n",
            "Iteration 190, loss = 0.13048572\n",
            "Iteration 191, loss = 0.13237269\n",
            "Iteration 192, loss = 0.12496430\n",
            "Iteration 193, loss = 0.12774481\n",
            "Iteration 194, loss = 0.13754501\n",
            "Iteration 195, loss = 0.14739476\n",
            "Iteration 196, loss = 0.14754348\n",
            "Iteration 197, loss = 0.14231204\n",
            "Iteration 198, loss = 0.13881042\n",
            "Iteration 199, loss = 0.14082380\n",
            "Iteration 200, loss = 0.12905878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 3.76850390\n",
            "Iteration 2, loss = 1.77761050\n",
            "Iteration 3, loss = 1.46429772\n",
            "Iteration 4, loss = 1.19714098\n",
            "Iteration 5, loss = 1.04432795\n",
            "Iteration 6, loss = 0.95297217\n",
            "Iteration 7, loss = 0.85168550\n",
            "Iteration 8, loss = 0.77347050\n",
            "Iteration 9, loss = 0.69112123\n",
            "Iteration 10, loss = 0.62778917\n",
            "Iteration 11, loss = 0.58938852\n",
            "Iteration 12, loss = 0.55444135\n",
            "Iteration 13, loss = 0.51828525\n",
            "Iteration 14, loss = 0.49010620\n",
            "Iteration 15, loss = 0.47205254\n",
            "Iteration 16, loss = 0.45642178\n",
            "Iteration 17, loss = 0.44099982\n",
            "Iteration 18, loss = 0.42423941\n",
            "Iteration 19, loss = 0.42405431\n",
            "Iteration 20, loss = 0.40587483\n",
            "Iteration 21, loss = 0.40891605\n",
            "Iteration 22, loss = 0.39811832\n",
            "Iteration 23, loss = 0.37900767\n",
            "Iteration 24, loss = 0.38018288\n",
            "Iteration 25, loss = 0.37009048\n",
            "Iteration 26, loss = 0.37104583\n",
            "Iteration 27, loss = 0.35467349\n",
            "Iteration 28, loss = 0.35328111\n",
            "Iteration 29, loss = 0.34977415\n",
            "Iteration 30, loss = 0.33673768\n",
            "Iteration 31, loss = 0.33549797\n",
            "Iteration 32, loss = 0.33093691\n",
            "Iteration 33, loss = 0.33050394\n",
            "Iteration 34, loss = 0.32523226\n",
            "Iteration 35, loss = 0.32805160\n",
            "Iteration 36, loss = 0.31217330\n",
            "Iteration 37, loss = 0.30669088\n",
            "Iteration 38, loss = 0.30344943\n",
            "Iteration 39, loss = 0.29798089\n",
            "Iteration 40, loss = 0.30023393\n",
            "Iteration 41, loss = 0.29333169\n",
            "Iteration 42, loss = 0.28907431\n",
            "Iteration 43, loss = 0.28460206\n",
            "Iteration 44, loss = 0.28984529\n",
            "Iteration 45, loss = 0.28399948\n",
            "Iteration 46, loss = 0.28333899\n",
            "Iteration 47, loss = 0.27652531\n",
            "Iteration 48, loss = 0.27513533\n",
            "Iteration 49, loss = 0.27547305\n",
            "Iteration 50, loss = 0.26676550\n",
            "Iteration 51, loss = 0.26954384\n",
            "Iteration 52, loss = 0.26416105\n",
            "Iteration 53, loss = 0.26155589\n",
            "Iteration 54, loss = 0.26192516\n",
            "Iteration 55, loss = 0.26244067\n",
            "Iteration 56, loss = 0.25032591\n",
            "Iteration 57, loss = 0.24967835\n",
            "Iteration 58, loss = 0.25384849\n",
            "Iteration 59, loss = 0.24998698\n",
            "Iteration 60, loss = 0.24240553\n",
            "Iteration 61, loss = 0.23981704\n",
            "Iteration 62, loss = 0.24290678\n",
            "Iteration 63, loss = 0.24411037\n",
            "Iteration 64, loss = 0.22931058\n",
            "Iteration 65, loss = 0.23078371\n",
            "Iteration 66, loss = 0.22999778\n",
            "Iteration 67, loss = 0.22213047\n",
            "Iteration 68, loss = 0.21954266\n",
            "Iteration 69, loss = 0.22241098\n",
            "Iteration 70, loss = 0.21473084\n",
            "Iteration 71, loss = 0.21773783\n",
            "Iteration 72, loss = 0.21615605\n",
            "Iteration 73, loss = 0.20842856\n",
            "Iteration 74, loss = 0.20905076\n",
            "Iteration 75, loss = 0.20882442\n",
            "Iteration 76, loss = 0.20491989\n",
            "Iteration 77, loss = 0.19740395\n",
            "Iteration 78, loss = 0.19115759\n",
            "Iteration 79, loss = 0.19054204\n",
            "Iteration 80, loss = 0.19386980\n",
            "Iteration 81, loss = 0.19023203\n",
            "Iteration 82, loss = 0.18476859\n",
            "Iteration 83, loss = 0.18488811\n",
            "Iteration 84, loss = 0.18124494\n",
            "Iteration 85, loss = 0.17950128\n",
            "Iteration 86, loss = 0.17900393\n",
            "Iteration 87, loss = 0.17314319\n",
            "Iteration 88, loss = 0.17506463\n",
            "Iteration 89, loss = 0.17225983\n",
            "Iteration 90, loss = 0.17713929\n",
            "Iteration 91, loss = 0.17291957\n",
            "Iteration 92, loss = 0.17033674\n",
            "Iteration 93, loss = 0.17521465\n",
            "Iteration 94, loss = 0.17011581\n",
            "Iteration 95, loss = 0.16820313\n",
            "Iteration 96, loss = 0.17015215\n",
            "Iteration 97, loss = 0.17054578\n",
            "Iteration 98, loss = 0.16359206\n",
            "Iteration 99, loss = 0.15940725\n",
            "Iteration 100, loss = 0.16409321\n",
            "Iteration 101, loss = 0.16612225\n",
            "Iteration 102, loss = 0.15690272\n",
            "Iteration 103, loss = 0.15884649\n",
            "Iteration 104, loss = 0.15911810\n",
            "Iteration 105, loss = 0.15598569\n",
            "Iteration 106, loss = 0.15289433\n",
            "Iteration 107, loss = 0.15305916\n",
            "Iteration 108, loss = 0.14956986\n",
            "Iteration 109, loss = 0.14805339\n",
            "Iteration 110, loss = 0.14907301\n",
            "Iteration 111, loss = 0.15193592\n",
            "Iteration 112, loss = 0.14518700\n",
            "Iteration 113, loss = 0.14860616\n",
            "Iteration 114, loss = 0.15269514\n",
            "Iteration 115, loss = 0.14497743\n",
            "Iteration 116, loss = 0.14440713\n",
            "Iteration 117, loss = 0.14753102\n",
            "Iteration 118, loss = 0.14509695\n",
            "Iteration 119, loss = 0.14700605\n",
            "Iteration 120, loss = 0.14207280\n",
            "Iteration 121, loss = 0.14238587\n",
            "Iteration 122, loss = 0.13808243\n",
            "Iteration 123, loss = 0.13685844\n",
            "Iteration 124, loss = 0.13557387\n",
            "Iteration 125, loss = 0.13463802\n",
            "Iteration 126, loss = 0.13582824\n",
            "Iteration 127, loss = 0.13843438\n",
            "Iteration 128, loss = 0.13336144\n",
            "Iteration 129, loss = 0.13987378\n",
            "Iteration 130, loss = 0.13170792\n",
            "Iteration 131, loss = 0.13814025\n",
            "Iteration 132, loss = 0.14114668\n",
            "Iteration 133, loss = 0.14603313\n",
            "Iteration 134, loss = 0.13396775\n",
            "Iteration 135, loss = 0.13988624\n",
            "Iteration 136, loss = 0.13434740\n",
            "Iteration 137, loss = 0.13441832\n",
            "Iteration 138, loss = 0.12730155\n",
            "Iteration 139, loss = 0.13116339\n",
            "Iteration 140, loss = 0.12866413\n",
            "Iteration 141, loss = 0.12907105\n",
            "Iteration 142, loss = 0.13241082\n",
            "Iteration 143, loss = 0.12685797\n",
            "Iteration 144, loss = 0.12610521\n",
            "Iteration 145, loss = 0.12698346\n",
            "Iteration 146, loss = 0.12420399\n",
            "Iteration 147, loss = 0.12174183\n",
            "Iteration 148, loss = 0.12225997\n",
            "Iteration 149, loss = 0.11746605\n",
            "Iteration 150, loss = 0.12550317\n",
            "Iteration 151, loss = 0.12054411\n",
            "Iteration 152, loss = 0.11756250\n",
            "Iteration 153, loss = 0.12478260\n",
            "Iteration 154, loss = 0.12914396\n",
            "Iteration 155, loss = 0.13020547\n",
            "Iteration 156, loss = 0.12198931\n",
            "Iteration 157, loss = 0.11861076\n",
            "Iteration 158, loss = 0.12128489\n",
            "Iteration 159, loss = 0.11451201\n",
            "Iteration 160, loss = 0.11717119\n",
            "Iteration 161, loss = 0.11925631\n",
            "Iteration 162, loss = 0.11907625\n",
            "Iteration 163, loss = 0.12177154\n",
            "Iteration 164, loss = 0.11734267\n",
            "Iteration 165, loss = 0.11598316\n",
            "Iteration 166, loss = 0.11360312\n",
            "Iteration 167, loss = 0.12206661\n",
            "Iteration 168, loss = 0.11408283\n",
            "Iteration 169, loss = 0.11224368\n",
            "Iteration 170, loss = 0.12143097\n",
            "Iteration 171, loss = 0.13302767\n",
            "Iteration 172, loss = 0.12181873\n",
            "Iteration 173, loss = 0.11572621\n",
            "Iteration 174, loss = 0.11028712\n",
            "Iteration 175, loss = 0.10845162\n",
            "Iteration 176, loss = 0.11350610\n",
            "Iteration 177, loss = 0.12266615\n",
            "Iteration 178, loss = 0.11579440\n",
            "Iteration 179, loss = 0.12385044\n",
            "Iteration 180, loss = 0.12885888\n",
            "Iteration 181, loss = 0.11461878\n",
            "Iteration 182, loss = 0.12382724\n",
            "Iteration 183, loss = 0.11563039\n",
            "Iteration 184, loss = 0.11471863\n",
            "Iteration 185, loss = 0.10703747\n",
            "Iteration 186, loss = 0.10996729\n",
            "Iteration 187, loss = 0.10639532\n",
            "Iteration 188, loss = 0.11314331\n",
            "Iteration 189, loss = 0.11133558\n",
            "Iteration 190, loss = 0.11591420\n",
            "Iteration 191, loss = 0.11352645\n",
            "Iteration 192, loss = 0.10803826\n",
            "Iteration 193, loss = 0.11533016\n",
            "Iteration 194, loss = 0.11096225\n",
            "Iteration 195, loss = 0.12339513\n",
            "Iteration 196, loss = 0.11712967\n",
            "Iteration 197, loss = 0.11912451\n",
            "Iteration 198, loss = 0.11986205\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.94723067\n",
            "Iteration 2, loss = 1.91943220\n",
            "Iteration 3, loss = 1.76591327\n",
            "Iteration 4, loss = 1.59837745\n",
            "Iteration 5, loss = 1.46165784\n",
            "Iteration 6, loss = 1.39647268\n",
            "Iteration 7, loss = 1.31010550\n",
            "Iteration 8, loss = 1.23319877\n",
            "Iteration 9, loss = 1.19233691\n",
            "Iteration 10, loss = 1.17706307\n",
            "Iteration 11, loss = 1.17181916\n",
            "Iteration 12, loss = 1.14100994\n",
            "Iteration 13, loss = 1.10115424\n",
            "Iteration 14, loss = 1.07657875\n",
            "Iteration 15, loss = 1.06306556\n",
            "Iteration 16, loss = 1.04671514\n",
            "Iteration 17, loss = 1.03001755\n",
            "Iteration 18, loss = 1.01393289\n",
            "Iteration 19, loss = 1.01026032\n",
            "Iteration 20, loss = 0.99394704\n",
            "Iteration 21, loss = 0.99274100\n",
            "Iteration 22, loss = 0.97960477\n",
            "Iteration 23, loss = 0.98332235\n",
            "Iteration 24, loss = 0.96065703\n",
            "Iteration 25, loss = 0.95414730\n",
            "Iteration 26, loss = 0.95594840\n",
            "Iteration 27, loss = 0.93809944\n",
            "Iteration 28, loss = 0.98155183\n",
            "Iteration 29, loss = 0.93402703\n",
            "Iteration 30, loss = 0.91610223\n",
            "Iteration 31, loss = 0.91588896\n",
            "Iteration 32, loss = 0.91102477\n",
            "Iteration 33, loss = 0.90889539\n",
            "Iteration 34, loss = 0.89969595\n",
            "Iteration 35, loss = 0.89485115\n",
            "Iteration 36, loss = 0.88953956\n",
            "Iteration 37, loss = 0.88414129\n",
            "Iteration 38, loss = 0.88146166\n",
            "Iteration 39, loss = 0.87212678\n",
            "Iteration 40, loss = 0.87773688\n",
            "Iteration 41, loss = 0.86430481\n",
            "Iteration 42, loss = 0.86691544\n",
            "Iteration 43, loss = 0.85823496\n",
            "Iteration 44, loss = 0.85110756\n",
            "Iteration 45, loss = 0.86516732\n",
            "Iteration 46, loss = 0.85286470\n",
            "Iteration 47, loss = 0.84424273\n",
            "Iteration 48, loss = 0.85262456\n",
            "Iteration 49, loss = 0.83383731\n",
            "Iteration 50, loss = 0.83676407\n",
            "Iteration 51, loss = 0.83224006\n",
            "Iteration 52, loss = 0.83147960\n",
            "Iteration 53, loss = 0.83270374\n",
            "Iteration 54, loss = 0.82631694\n",
            "Iteration 55, loss = 0.82101592\n",
            "Iteration 56, loss = 0.81941530\n",
            "Iteration 57, loss = 0.82665003\n",
            "Iteration 58, loss = 0.81508203\n",
            "Iteration 59, loss = 0.80741256\n",
            "Iteration 60, loss = 0.80382698\n",
            "Iteration 61, loss = 0.81147555\n",
            "Iteration 62, loss = 0.80043202\n",
            "Iteration 63, loss = 0.79563455\n",
            "Iteration 64, loss = 0.79265819\n",
            "Iteration 65, loss = 0.80129673\n",
            "Iteration 66, loss = 0.78900950\n",
            "Iteration 67, loss = 0.78987505\n",
            "Iteration 68, loss = 0.78795004\n",
            "Iteration 69, loss = 0.78929257\n",
            "Iteration 70, loss = 0.78276035\n",
            "Iteration 71, loss = 0.77884277\n",
            "Iteration 72, loss = 0.79773834\n",
            "Iteration 73, loss = 0.82134543\n",
            "Iteration 74, loss = 0.80338838\n",
            "Iteration 75, loss = 0.78027278\n",
            "Iteration 76, loss = 0.79430410\n",
            "Iteration 77, loss = 0.76663877\n",
            "Iteration 78, loss = 0.79005427\n",
            "Iteration 79, loss = 0.75644638\n",
            "Iteration 80, loss = 0.75876733\n",
            "Iteration 81, loss = 0.76442750\n",
            "Iteration 82, loss = 0.75597430\n",
            "Iteration 83, loss = 0.74624019\n",
            "Iteration 84, loss = 0.74796452\n",
            "Iteration 85, loss = 0.74300924\n",
            "Iteration 86, loss = 0.75154657\n",
            "Iteration 87, loss = 0.74379555\n",
            "Iteration 88, loss = 0.73809909\n",
            "Iteration 89, loss = 0.75081621\n",
            "Iteration 90, loss = 0.73984015\n",
            "Iteration 91, loss = 0.74158483\n",
            "Iteration 92, loss = 0.73132002\n",
            "Iteration 93, loss = 0.72950981\n",
            "Iteration 94, loss = 0.73639852\n",
            "Iteration 95, loss = 0.72600760\n",
            "Iteration 96, loss = 0.72970854\n",
            "Iteration 97, loss = 0.72221081\n",
            "Iteration 98, loss = 0.71933514\n",
            "Iteration 99, loss = 0.71531779\n",
            "Iteration 100, loss = 0.75208360\n",
            "Iteration 101, loss = 0.71574242\n",
            "Iteration 102, loss = 0.71563893\n",
            "Iteration 103, loss = 0.71656129\n",
            "Iteration 104, loss = 0.74238418\n",
            "Iteration 105, loss = 0.71029854\n",
            "Iteration 106, loss = 0.70485230\n",
            "Iteration 107, loss = 0.70643118\n",
            "Iteration 108, loss = 0.70650506\n",
            "Iteration 109, loss = 0.69941377\n",
            "Iteration 110, loss = 0.70385522\n",
            "Iteration 111, loss = 0.70017764\n",
            "Iteration 112, loss = 0.68785823\n",
            "Iteration 113, loss = 0.68598093\n",
            "Iteration 114, loss = 0.68913849\n",
            "Iteration 115, loss = 0.67982530\n",
            "Iteration 116, loss = 0.68304320\n",
            "Iteration 117, loss = 0.67964327\n",
            "Iteration 118, loss = 0.69115394\n",
            "Iteration 119, loss = 0.72103196\n",
            "Iteration 120, loss = 0.67566710\n",
            "Iteration 121, loss = 0.67632931\n",
            "Iteration 122, loss = 0.67505868\n",
            "Iteration 123, loss = 0.67192626\n",
            "Iteration 124, loss = 0.66839341\n",
            "Iteration 125, loss = 0.67560962\n",
            "Iteration 126, loss = 0.67309154\n",
            "Iteration 127, loss = 0.67159838\n",
            "Iteration 128, loss = 0.66550779\n",
            "Iteration 129, loss = 0.67128132\n",
            "Iteration 130, loss = 0.66894968\n",
            "Iteration 131, loss = 0.66562925\n",
            "Iteration 132, loss = 0.66366138\n",
            "Iteration 133, loss = 0.66270016\n",
            "Iteration 134, loss = 0.66849529\n",
            "Iteration 135, loss = 0.65448299\n",
            "Iteration 136, loss = 0.65751100\n",
            "Iteration 137, loss = 0.65044300\n",
            "Iteration 138, loss = 0.65489938\n",
            "Iteration 139, loss = 0.65209016\n",
            "Iteration 140, loss = 0.65272575\n",
            "Iteration 141, loss = 0.65400815\n",
            "Iteration 142, loss = 0.65051787\n",
            "Iteration 143, loss = 0.65900305\n",
            "Iteration 144, loss = 0.65130608\n",
            "Iteration 145, loss = 0.64849496\n",
            "Iteration 146, loss = 0.64501459\n",
            "Iteration 147, loss = 0.64541442\n",
            "Iteration 148, loss = 0.64890676\n",
            "Iteration 149, loss = 0.65017827\n",
            "Iteration 150, loss = 0.64407881\n",
            "Iteration 151, loss = 0.64536105\n",
            "Iteration 152, loss = 0.64209094\n",
            "Iteration 153, loss = 0.63997457\n",
            "Iteration 154, loss = 0.64037490\n",
            "Iteration 155, loss = 0.64606557\n",
            "Iteration 156, loss = 0.63660459\n",
            "Iteration 157, loss = 0.64861333\n",
            "Iteration 158, loss = 0.63993431\n",
            "Iteration 159, loss = 0.64217528\n",
            "Iteration 160, loss = 0.63118613\n",
            "Iteration 161, loss = 0.69288392\n",
            "Iteration 162, loss = 0.68199114\n",
            "Iteration 163, loss = 0.64641691\n",
            "Iteration 164, loss = 0.63791409\n",
            "Iteration 165, loss = 0.63902689\n",
            "Iteration 166, loss = 0.63295788\n",
            "Iteration 167, loss = 0.62958893\n",
            "Iteration 168, loss = 0.62904311\n",
            "Iteration 169, loss = 0.64140797\n",
            "Iteration 170, loss = 0.62657615\n",
            "Iteration 171, loss = 0.63188284\n",
            "Iteration 172, loss = 0.62608235\n",
            "Iteration 173, loss = 0.62798740\n",
            "Iteration 174, loss = 0.62620957\n",
            "Iteration 175, loss = 0.63031221\n",
            "Iteration 176, loss = 0.62642335\n",
            "Iteration 177, loss = 0.63055495\n",
            "Iteration 178, loss = 0.62000994\n",
            "Iteration 179, loss = 0.62521972\n",
            "Iteration 180, loss = 0.61588438\n",
            "Iteration 181, loss = 0.62455083\n",
            "Iteration 182, loss = 0.62401096\n",
            "Iteration 183, loss = 0.62055354\n",
            "Iteration 184, loss = 0.62041079\n",
            "Iteration 185, loss = 0.62469031\n",
            "Iteration 186, loss = 0.61824342\n",
            "Iteration 187, loss = 0.61814255\n",
            "Iteration 188, loss = 0.62189186\n",
            "Iteration 189, loss = 0.61703701\n",
            "Iteration 190, loss = 0.61823560\n",
            "Iteration 191, loss = 0.61641461\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.78958066\n",
            "Iteration 2, loss = 1.84332466\n",
            "Iteration 3, loss = 1.75287487\n",
            "Iteration 4, loss = 1.64339057\n",
            "Iteration 5, loss = 1.49826494\n",
            "Iteration 6, loss = 1.32543531\n",
            "Iteration 7, loss = 1.26052935\n",
            "Iteration 8, loss = 1.19730375\n",
            "Iteration 9, loss = 1.16530714\n",
            "Iteration 10, loss = 1.12457987\n",
            "Iteration 11, loss = 1.20393775\n",
            "Iteration 12, loss = 1.13772518\n",
            "Iteration 13, loss = 1.05887883\n",
            "Iteration 14, loss = 1.05387771\n",
            "Iteration 15, loss = 1.09884543\n",
            "Iteration 16, loss = 1.01130549\n",
            "Iteration 17, loss = 0.97794612\n",
            "Iteration 18, loss = 0.98674563\n",
            "Iteration 19, loss = 0.94978241\n",
            "Iteration 20, loss = 0.86881916\n",
            "Iteration 21, loss = 0.88907363\n",
            "Iteration 22, loss = 0.87019897\n",
            "Iteration 23, loss = 0.82333530\n",
            "Iteration 24, loss = 0.79739341\n",
            "Iteration 25, loss = 0.75941621\n",
            "Iteration 26, loss = 0.73844093\n",
            "Iteration 27, loss = 0.71804917\n",
            "Iteration 28, loss = 0.70974238\n",
            "Iteration 29, loss = 0.70192605\n",
            "Iteration 30, loss = 0.68091544\n",
            "Iteration 31, loss = 0.67855215\n",
            "Iteration 32, loss = 0.65547830\n",
            "Iteration 33, loss = 0.64055057\n",
            "Iteration 34, loss = 0.62087947\n",
            "Iteration 35, loss = 0.61512013\n",
            "Iteration 36, loss = 0.58994740\n",
            "Iteration 37, loss = 0.57725363\n",
            "Iteration 38, loss = 0.56083254\n",
            "Iteration 39, loss = 0.53247795\n",
            "Iteration 40, loss = 0.52395998\n",
            "Iteration 41, loss = 0.51555024\n",
            "Iteration 42, loss = 0.50500493\n",
            "Iteration 43, loss = 0.48696448\n",
            "Iteration 44, loss = 0.47964805\n",
            "Iteration 45, loss = 0.49678074\n",
            "Iteration 46, loss = 0.45749508\n",
            "Iteration 47, loss = 0.44898246\n",
            "Iteration 48, loss = 0.44423077\n",
            "Iteration 49, loss = 0.42553962\n",
            "Iteration 50, loss = 0.42845340\n",
            "Iteration 51, loss = 0.42506266\n",
            "Iteration 52, loss = 0.41106553\n",
            "Iteration 53, loss = 0.40553841\n",
            "Iteration 54, loss = 0.39914175\n",
            "Iteration 55, loss = 0.39787562\n",
            "Iteration 56, loss = 0.38902427\n",
            "Iteration 57, loss = 0.39064865\n",
            "Iteration 58, loss = 0.38521734\n",
            "Iteration 59, loss = 0.37418137\n",
            "Iteration 60, loss = 0.37475815\n",
            "Iteration 61, loss = 0.36873459\n",
            "Iteration 62, loss = 0.41373056\n",
            "Iteration 63, loss = 0.37163706\n",
            "Iteration 64, loss = 0.35044245\n",
            "Iteration 65, loss = 0.34055784\n",
            "Iteration 66, loss = 0.33887055\n",
            "Iteration 67, loss = 0.33475547\n",
            "Iteration 68, loss = 0.33068064\n",
            "Iteration 69, loss = 0.32668941\n",
            "Iteration 70, loss = 0.31395950\n",
            "Iteration 71, loss = 0.31031549\n",
            "Iteration 72, loss = 0.30806378\n",
            "Iteration 73, loss = 0.29681909\n",
            "Iteration 74, loss = 0.30196550\n",
            "Iteration 75, loss = 0.28799893\n",
            "Iteration 76, loss = 0.28987227\n",
            "Iteration 77, loss = 0.28173593\n",
            "Iteration 78, loss = 0.28135159\n",
            "Iteration 79, loss = 0.27571598\n",
            "Iteration 80, loss = 0.27909004\n",
            "Iteration 81, loss = 0.27298217\n",
            "Iteration 82, loss = 0.26419202\n",
            "Iteration 83, loss = 0.26298372\n",
            "Iteration 84, loss = 0.26404945\n",
            "Iteration 85, loss = 0.25454575\n",
            "Iteration 86, loss = 0.25542825\n",
            "Iteration 87, loss = 0.25287562\n",
            "Iteration 88, loss = 0.24656084\n",
            "Iteration 89, loss = 0.24443416\n",
            "Iteration 90, loss = 0.23912273\n",
            "Iteration 91, loss = 0.24059626\n",
            "Iteration 92, loss = 0.23390550\n",
            "Iteration 93, loss = 0.23158441\n",
            "Iteration 94, loss = 0.23033905\n",
            "Iteration 95, loss = 0.22981535\n",
            "Iteration 96, loss = 0.22672579\n",
            "Iteration 97, loss = 0.22817879\n",
            "Iteration 98, loss = 0.22549015\n",
            "Iteration 99, loss = 0.22410308\n",
            "Iteration 100, loss = 0.21807304\n",
            "Iteration 101, loss = 0.21808289\n",
            "Iteration 102, loss = 0.21615649\n",
            "Iteration 103, loss = 0.22603864\n",
            "Iteration 104, loss = 0.21664748\n",
            "Iteration 105, loss = 0.20756281\n",
            "Iteration 106, loss = 0.20914561\n",
            "Iteration 107, loss = 0.20992816\n",
            "Iteration 108, loss = 0.20131738\n",
            "Iteration 109, loss = 0.20591615\n",
            "Iteration 110, loss = 0.20436327\n",
            "Iteration 111, loss = 0.20101210\n",
            "Iteration 112, loss = 0.19924162\n",
            "Iteration 113, loss = 0.20036058\n",
            "Iteration 114, loss = 0.19961817\n",
            "Iteration 115, loss = 0.19829258\n",
            "Iteration 116, loss = 0.20075554\n",
            "Iteration 117, loss = 0.19656767\n",
            "Iteration 118, loss = 0.19511890\n",
            "Iteration 119, loss = 0.19063844\n",
            "Iteration 120, loss = 0.19259945\n",
            "Iteration 121, loss = 0.18857882\n",
            "Iteration 122, loss = 0.18699074\n",
            "Iteration 123, loss = 0.19502432\n",
            "Iteration 124, loss = 0.19154447\n",
            "Iteration 125, loss = 0.18581110\n",
            "Iteration 126, loss = 0.18158379\n",
            "Iteration 127, loss = 0.18639560\n",
            "Iteration 128, loss = 0.18661792\n",
            "Iteration 129, loss = 0.18603219\n",
            "Iteration 130, loss = 0.18397985\n",
            "Iteration 131, loss = 0.19185284\n",
            "Iteration 132, loss = 0.18090210\n",
            "Iteration 133, loss = 0.17954892\n",
            "Iteration 134, loss = 0.17604933\n",
            "Iteration 135, loss = 0.17735665\n",
            "Iteration 136, loss = 0.17626501\n",
            "Iteration 137, loss = 0.17830012\n",
            "Iteration 138, loss = 0.17444567\n",
            "Iteration 139, loss = 0.17612716\n",
            "Iteration 140, loss = 0.17898088\n",
            "Iteration 141, loss = 0.17408500\n",
            "Iteration 142, loss = 0.17392336\n",
            "Iteration 143, loss = 0.17424401\n",
            "Iteration 144, loss = 0.17676186\n",
            "Iteration 145, loss = 0.17075721\n",
            "Iteration 146, loss = 0.16487855\n",
            "Iteration 147, loss = 0.16746271\n",
            "Iteration 148, loss = 0.16812817\n",
            "Iteration 149, loss = 0.16469208\n",
            "Iteration 150, loss = 0.16941060\n",
            "Iteration 151, loss = 0.16274500\n",
            "Iteration 152, loss = 0.16428222\n",
            "Iteration 153, loss = 0.16159196\n",
            "Iteration 154, loss = 0.16014310\n",
            "Iteration 155, loss = 0.16154977\n",
            "Iteration 156, loss = 0.16055833\n",
            "Iteration 157, loss = 0.15767997\n",
            "Iteration 158, loss = 0.16095469\n",
            "Iteration 159, loss = 0.16673742\n",
            "Iteration 160, loss = 0.16860490\n",
            "Iteration 161, loss = 0.16485442\n",
            "Iteration 162, loss = 0.15713532\n",
            "Iteration 163, loss = 0.15493646\n",
            "Iteration 164, loss = 0.15200558\n",
            "Iteration 165, loss = 0.15652123\n",
            "Iteration 166, loss = 0.15525228\n",
            "Iteration 167, loss = 0.15599941\n",
            "Iteration 168, loss = 0.15608884\n",
            "Iteration 169, loss = 0.15664893\n",
            "Iteration 170, loss = 0.15555753\n",
            "Iteration 171, loss = 0.15699273\n",
            "Iteration 172, loss = 0.15417356\n",
            "Iteration 173, loss = 0.15944696\n",
            "Iteration 174, loss = 0.15470470\n",
            "Iteration 175, loss = 0.15485365\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.60302203\n",
            "Iteration 2, loss = 1.44993047\n",
            "Iteration 3, loss = 1.27332578\n",
            "Iteration 4, loss = 1.14798937\n",
            "Iteration 5, loss = 1.06230086\n",
            "Iteration 6, loss = 0.99991829\n",
            "Iteration 7, loss = 0.90596667\n",
            "Iteration 8, loss = 0.81563167\n",
            "Iteration 9, loss = 0.71721765\n",
            "Iteration 10, loss = 0.92725731\n",
            "Iteration 11, loss = 0.78672982\n",
            "Iteration 12, loss = 0.71841501\n",
            "Iteration 13, loss = 0.68873642\n",
            "Iteration 14, loss = 0.67099417\n",
            "Iteration 15, loss = 0.59977482\n",
            "Iteration 16, loss = 0.59133382\n",
            "Iteration 17, loss = 0.54618992\n",
            "Iteration 18, loss = 0.56682507\n",
            "Iteration 19, loss = 0.54261152\n",
            "Iteration 20, loss = 0.54986901\n",
            "Iteration 21, loss = 0.49137846\n",
            "Iteration 22, loss = 0.47979150\n",
            "Iteration 23, loss = 0.46877128\n",
            "Iteration 24, loss = 0.60517754\n",
            "Iteration 25, loss = 0.63230754\n",
            "Iteration 26, loss = 0.59858303\n",
            "Iteration 27, loss = 0.61378638\n",
            "Iteration 28, loss = 0.76170572\n",
            "Iteration 29, loss = 0.67976662\n",
            "Iteration 30, loss = 0.91422685\n",
            "Iteration 31, loss = 0.80120068\n",
            "Iteration 32, loss = 0.75247919\n",
            "Iteration 33, loss = 0.71795442\n",
            "Iteration 34, loss = 0.71796946\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 4.00840721\n",
            "Iteration 2, loss = 1.70762956\n",
            "Iteration 3, loss = 1.57101299\n",
            "Iteration 4, loss = 1.44860854\n",
            "Iteration 5, loss = 1.34194091\n",
            "Iteration 6, loss = 1.24765957\n",
            "Iteration 7, loss = 1.18500294\n",
            "Iteration 8, loss = 1.13310720\n",
            "Iteration 9, loss = 1.01704986\n",
            "Iteration 10, loss = 0.86868984\n",
            "Iteration 11, loss = 0.79840514\n",
            "Iteration 12, loss = 0.81243008\n",
            "Iteration 13, loss = 0.91963189\n",
            "Iteration 14, loss = 0.84207869\n",
            "Iteration 15, loss = 0.76499460\n",
            "Iteration 16, loss = 0.69494639\n",
            "Iteration 17, loss = 0.70026871\n",
            "Iteration 18, loss = 0.61185968\n",
            "Iteration 19, loss = 0.57283562\n",
            "Iteration 20, loss = 0.61634278\n",
            "Iteration 21, loss = 0.52514621\n",
            "Iteration 22, loss = 0.51269448\n",
            "Iteration 23, loss = 0.48792828\n",
            "Iteration 24, loss = 0.44114140\n",
            "Iteration 25, loss = 0.41254155\n",
            "Iteration 26, loss = 0.43398363\n",
            "Iteration 27, loss = 0.39924439\n",
            "Iteration 28, loss = 0.43555442\n",
            "Iteration 29, loss = 0.62305355\n",
            "Iteration 30, loss = 0.58048889\n",
            "Iteration 31, loss = 0.46405228\n",
            "Iteration 32, loss = 0.39347723\n",
            "Iteration 33, loss = 0.37895181\n",
            "Iteration 34, loss = 0.35094742\n",
            "Iteration 35, loss = 0.33894730\n",
            "Iteration 36, loss = 0.39110402\n",
            "Iteration 37, loss = 0.36104041\n",
            "Iteration 38, loss = 0.33849312\n",
            "Iteration 39, loss = 0.32090955\n",
            "Iteration 40, loss = 0.31990888\n",
            "Iteration 41, loss = 0.30799399\n",
            "Iteration 42, loss = 0.30512328\n",
            "Iteration 43, loss = 0.30127193\n",
            "Iteration 44, loss = 0.29442397\n",
            "Iteration 45, loss = 0.29166773\n",
            "Iteration 46, loss = 0.28731702\n",
            "Iteration 47, loss = 0.28431045\n",
            "Iteration 48, loss = 0.57769683\n",
            "Iteration 49, loss = 0.52370831\n",
            "Iteration 50, loss = 0.93092558\n",
            "Iteration 51, loss = 1.16497856\n",
            "Iteration 52, loss = 0.99300339\n",
            "Iteration 53, loss = 0.89491210\n",
            "Iteration 54, loss = 0.85119943\n",
            "Iteration 55, loss = 1.03033834\n",
            "Iteration 56, loss = 0.95754740\n",
            "Iteration 57, loss = 0.88519885\n",
            "Iteration 58, loss = 0.87713960\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 3.83625462\n",
            "Iteration 2, loss = 1.86101666\n",
            "Iteration 3, loss = 1.80006367\n",
            "Iteration 4, loss = 1.73441042\n",
            "Iteration 5, loss = 1.70349784\n",
            "Iteration 6, loss = 1.58953317\n",
            "Iteration 7, loss = 1.54421875\n",
            "Iteration 8, loss = 1.49353062\n",
            "Iteration 9, loss = 1.52622796\n",
            "Iteration 10, loss = 1.46089728\n",
            "Iteration 11, loss = 1.37965429\n",
            "Iteration 12, loss = 1.41871947\n",
            "Iteration 13, loss = 1.34702076\n",
            "Iteration 14, loss = 1.28059140\n",
            "Iteration 15, loss = 1.21871440\n",
            "Iteration 16, loss = 1.16980332\n",
            "Iteration 17, loss = 1.12392006\n",
            "Iteration 18, loss = 1.10457788\n",
            "Iteration 19, loss = 1.10134477\n",
            "Iteration 20, loss = 1.05267503\n",
            "Iteration 21, loss = 1.06049618\n",
            "Iteration 22, loss = 1.25946467\n",
            "Iteration 23, loss = 1.21081213\n",
            "Iteration 24, loss = 1.25364891\n",
            "Iteration 25, loss = 1.15022689\n",
            "Iteration 26, loss = 1.21391531\n",
            "Iteration 27, loss = 1.10899836\n",
            "Iteration 28, loss = 1.12201756\n",
            "Iteration 29, loss = 1.08857549\n",
            "Iteration 30, loss = 1.07920547\n",
            "Iteration 31, loss = 1.06512298\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 4.12389282\n",
            "Iteration 2, loss = 1.72080928\n",
            "Iteration 3, loss = 1.50978258\n",
            "Iteration 4, loss = 1.37368081\n",
            "Iteration 5, loss = 1.28380193\n",
            "Iteration 6, loss = 1.21141459\n",
            "Iteration 7, loss = 1.34401384\n",
            "Iteration 8, loss = 1.26303400\n",
            "Iteration 9, loss = 1.25700583\n",
            "Iteration 10, loss = 1.20133258\n",
            "Iteration 11, loss = 1.12590377\n",
            "Iteration 12, loss = 1.11667972\n",
            "Iteration 13, loss = 1.07600112\n",
            "Iteration 14, loss = 1.00979382\n",
            "Iteration 15, loss = 0.98863533\n",
            "Iteration 16, loss = 0.96014873\n",
            "Iteration 17, loss = 0.93831956\n",
            "Iteration 18, loss = 0.90144044\n",
            "Iteration 19, loss = 1.00531553\n",
            "Iteration 20, loss = 0.98535170\n",
            "Iteration 21, loss = 0.96653579\n",
            "Iteration 22, loss = 0.96771238\n",
            "Iteration 23, loss = 0.91528576\n",
            "Iteration 24, loss = 0.89069833\n",
            "Iteration 25, loss = 0.83663543\n",
            "Iteration 26, loss = 0.83291645\n",
            "Iteration 27, loss = 0.79109493\n",
            "Iteration 28, loss = 0.74233859\n",
            "Iteration 29, loss = 0.71842695\n",
            "Iteration 30, loss = 0.68081217\n",
            "Iteration 31, loss = 1.06738567\n",
            "Iteration 32, loss = 1.00023528\n",
            "Iteration 33, loss = 0.94496365\n",
            "Iteration 34, loss = 1.21858246\n",
            "Iteration 35, loss = 1.00949627\n",
            "Iteration 36, loss = 0.93604789\n",
            "Iteration 37, loss = 0.86660029\n",
            "Iteration 38, loss = 0.85019698\n",
            "Iteration 39, loss = 0.79557338\n",
            "Iteration 40, loss = 0.77666864\n",
            "Iteration 41, loss = 0.74595220\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Model Neural Net score: 0.7648 and std dev: 0.1148\n",
            "Validation variance score: 0.7114\n",
            "Test variance score: 0.7583\n",
            "Model XGB score: 0.9217 and std dev: 0.0078\n",
            "Validation variance score: 0.8294\n",
            "Test variance score: 0.8433\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 408 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.8min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.8min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 487 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 488 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 489 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 489 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 489 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 489 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.1min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 490 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 491 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 490 seconds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max_iter reached after 491 seconds\n",
            "Model LR score: 0.8699 and std dev: 0.0113\n",
            "Validation variance score: 0.7383\n",
            "Test variance score: 0.7747\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  \"the coef_ did not converge\", ConvergenceWarning)\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  8.2min finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "Model RF score: 0.6305 and std dev: 0.0241\n",
            "Validation variance score: 0.0167\n",
            "Test variance score: 0.0761\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4AVTyI6U32hZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Validation and results"
      ]
    },
    {
      "metadata": {
        "id": "jAo-dhkX32ha",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_validation(results):\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.boxplot(results[:,1])\n",
        "    ax.set_xticklabels(results[:,0])\n",
        "    ax.set_title('Model CV score')\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(results[:,0], results[:,2])\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Var score')\n",
        "    plt.title('Models variance score')\n",
        "    plt.show()\n",
        "    \n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(results[:,0], results[:,3])\n",
        "    plt.xlabel('Models')\n",
        "    plt.ylabel('Var score')\n",
        "    plt.title('Test variance score')\n",
        "    plt.show()\n",
        "    \n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "89ESNrfW32hc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np_results=np.array(results, object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIVOqo1K32he",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "outputId": "a19de34e-47d3-4a8e-e456-332a32db1e36"
      },
      "cell_type": "code",
      "source": [
        "status = model_validation(np_results)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAE+CAYAAAC6Iqj0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFMCAYAAADiATSNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XlcVfW+//H3ZjIVMlC2mlqa5YSZ\nerqWSeIlFDQNU3yIInjUHMqTV72eq+KAmTiUMw7n2HBM8KgnD3lvg0Nmg1NOjwa1m5UViaaAIIIT\noOv3hz/XjdiIHdvgF1/Pf2LttdZ3f9Z3uXvv9V1rr+WwLMsSAAAwhkdFFwAAAH4bwhsAAMMQ3gAA\nGIbwBgDAMIQ3AACGIbwBADAM4Q38C5o2bapRo0aVeH3SpElq2rTpb25v0qRJSkpKuu4yqamp+uMf\n//ib274ZKSkpWrhwYbm+J4CyeVV0AYCpjhw5ovz8fPn6+kqSCgoKdPDgwQqu6vc1YMCAii4BgAsc\neQP/okceeUTvv/++Pb1jxw49+OCDxZbZuHGjunfvroiICMXFxemnn36SJOXk5Gjw4MEKDQ3VsGHD\nlJeXZ6/z3XffacCAAQoPD1ePHj1cfiHYu3evnn76aXXr1k1du3bVxo0bi82/cuWKgoODdejQIfu1\nlStXasyYMZKkpUuXKjw8XGFhYRo+fLjOnj0rSUpKStLkyZMVFRWllStXKikpSZMmTZIkff/99+rX\nr5+6du2qzp0765133rHbbtq0qTZs2KCePXsqODhYK1eutOetWLFCTzzxhMLDwzVr1ixduy/UunXr\nFBERodDQUI0dO1YXL14ssZ3nzp3TyJEj1bVrVz3xxBOaPHmyCgsLr9vuqlWr1K1bN0VEROjZZ59V\ndna2JGnChAmaNWuWevTooY0bN6qgoEAzZsxQeHi4QkND9Ze//KXkTgZuVRaA36xJkybWrl27rMGD\nB9uvjR071vrkk0+sJk2aWJZlWcePH7f+8Ic/WD/++KNlWZb12muvWQMHDrQsy7LmzJljjR071rIs\nyzp27JjVpk0ba/Hixdbly5etLl26WP/4xz8sy7Ks/fv3W8HBwVZhYaH1z3/+016/V69e1p49eyzL\nsqwffvjBbuuXEhISrEWLFtnTMTEx1ubNm62DBw9a7du3t/Ly8qzLly9bf/zjH62lS5dalmVZixcv\ntoKDg63Tp0/b0/Hx8ZZlWdbw4cOtv/71r5ZlWdbevXutVq1aWQUFBXZ/vPzyy5ZlWdYXX3xhPfjg\ng1ZRUZG1b98+q3PnzlZeXp516dIlq3fv3tZ7771n7du3z2rfvr118uRJy7Isa8qUKdbs2bNLbENK\nSoo1YcIEy7Isq7Cw0Jo6dar11VdfldruZ599ZnXs2NHKysqyLMuypk+fbtc/fvx4q0ePHtbFixct\ny7KsJUuWWAMHDrQuXbpknTt3zurZs6e1bdu26+944BbBkTfwL2rXrp2+/fZbnT59WhcuXNBnn32m\n9u3b2/N37typRx55RPfee68kqU+fPtqzZ4+Kioq0f/9+de3aVZJUv359tWvXTtLVo9vTp08rKipK\nkvSHP/xBAQEB+uyzz4q9d82aNbVhwwYdPXpUDRs21Lx580rUFx4erm3btkmSsrOz9fXXXyskJEQt\nW7bURx99JF9fX3l4eKhNmzY6duyYvd5DDz2kgICAEu0tW7ZMQ4YMseu6dOmSMjMz7fmRkZGSpKCg\nIF26dEmnT5/WJ598opCQEPn6+srHx0fJycnq0qWLtm3bpm7duql27dqSpH79+mnLli0l3vPatu/Y\nsUNXrlzRCy+8oObNm5fa7kcffaTw8HDVrFnT7vOdO3fa7bVv315VqlSRJH344Yfq37+/fHx8VK1a\nNUVGRrqsAbgVcc4b+Bd5enqqS5cu2rhxowICAhQcHCwvr//7SOXk5OjOO++0p/38/GRZlnJycpSb\nmys/Pz973rXlzp49q4sXL9rBLkn5+fk6c+ZMsfeeOXOmli9frkGDBumOO+7Q2LFjFRERUWyZdu3a\n6dSpUzpx4oR27dqlkJAQValSRRcuXNCsWbO0Z88eSVJubq46depkr1ejRg2X27t9+3YtX75cOTk5\ncjgcsixLV65cKbZ91/pFujp0n5OTI6fTaS9TtWpVSVJeXp7ef/997dixQ5JkWZY9HP5LXbt2VW5u\nrhYtWqTvv/9eTz31lCZOnFhqu9nZ2cVev/POO3X69GmX25aXl6dZs2Zp/vz5kq5es9CqVSuX2w7c\naghv4CZ069ZNCxYskL+/v/r3719sXs2aNYsdMefm5srDw0P+/v668847i53nzs7OVoMGDeR0OlW9\nenVt2rSpxHulpqbaf9eqVUtTpkzRlClTtGPHDj3//PN6/PHHVb16dXsZT09PhYWF6cMPP9T27dvt\no/k33nhDP/74o1JTU1W9enUtWLBAp06duu52FhYWavTo0Vq4cKFCQkJuOOj8/f2Vk5NjT1/72+l0\n6umnn9b48ePLbCM6OlrR0dE6deqUnn/+eW3YsKHUdmvVqlXsi86ZM2dUq1Ytl+06nU4NHjxY//7v\n/15mDcCthmFz4Ca0adNGGRkZ+vbbb+2h72s6dOig/fv320PSa9euVYcOHeTl5aXWrVtr69atkqSf\nfvpJBw4ckCTVq1dPderUscM7OztbY8eO1fnz5+12CwsLFRsbq4yMDElXh6m9vLzk4VHy43xt6Pzg\nwYPq2LGjJOn06dO67777VL16dR0/flwff/xxsfZduXDhgs6fP6+WLVtKuvoFwNvbu8z1QkNDtW3b\nNuXm5qqoqEgjR47Ujh07FBoaqi1bttgXk23dulUrVqwosf7SpUu1fv16SVLt2rVVv359ORyOUtvt\n1KmT3n//fTvM165dq5CQEJe1PfHEE3rzzTd1+fJlWZalZcuW6ZNPPrnu9gC3Co68gZvgcDjUuXNn\nXbhwoUR41qlTRzNmzNBzzz2nwsJC1a9fXy+++KIkafjw4RozZoxCQ0PVuHFjdenSxW5v/vz5mjZt\nmhYuXCgPDw8NGjRI1apVs9v19vZWVFSU/ZtvDw8PTZ482R46/qVHH31U//mf/6mOHTvKx8dH0tUj\n2VGjRik8PFxNmzbVhAkT9Pzzzxe7QvzX7rzzTj3zzDPq2bOnatasqWeffVZhYWEaMWJEsavOf611\n69YaMmSIevbsKR8fHz3++OPq3r27HA6HRowYodjYWF25ckU1a9bUCy+8UGL9yMhITZw4Ua+88ooc\nDoceeughRUZGysfHp9R2hw0bppiYGF25ckXNmzfXtGnTXNbWv39/paen68knn5RlWWrZsqUGDhxY\n6rYAtxKHZfE8bwAATMKwOQAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBhjfiqWmZlX9kK3MH//asrJ\nuf5vYm9H9Itr9Itr9Itr9ItrlaFfAgP9XL7OkXc58fLyrOgSbkn0i2v0i2v0i2v0i2uVuV8IbwAA\nDEN4AwBgGMIbAADDEN4AABiG8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBjCGwAAwxjz\nYBKUj8Gzt1V0CW71+oTQii4BAG4aR94AABiG8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMA\nYBjCGwAAwxDeAAAYhvAGAMAwhDcAAIYhvAEAMAzhDQCAYW7bp4rx9CwAgKk48gYAwDCENwAAhiG8\nAQAwDOENAIBhCG8AAAxz215tDuDm8asNoGJw5A0AgGEIbwAADEN4AwBgGMIbAADDuPWCtZkzZ+qL\nL76Qw+FQfHy8WrVqZc9bvXq1/ud//kceHh5q2bKlJk2a5M5SAACoNNx25L13716lpaVp3bp1SkxM\nVGJioj0vPz9fr732mlavXq01a9bo6NGj+vzzz91VCgAAlYrbwnv37t0KCwuTJDVu3Fi5ubnKz8+X\nJHl7e8vb21vnz59XUVGRLly4oBo1arirFAAAKhW3DZtnZWUpKCjIng4ICFBmZqZ8fX1VpUoVjRw5\nUmFhYapSpYqefPJJNWrU6Lrt+ftXk5eXp7vKrXQCA/0quoRbkkn9YlKtlZVJ+8CkWstTZe2XcrtJ\ni2VZ9t/5+fn661//qk2bNsnX11cDBw7U119/rWbNmpW6fk7O+fIos9LIzMyr6BJuSab0S2CgnzG1\nVmam7AP+vbhWGfqltC8fbhs2dzqdysrKsqczMjIUGBgoSTp69KgaNGiggIAA+fj46OGHH9ahQ4fc\nVQoAAJWK28K7Q4cO2rx5syTp8OHDcjqd8vX1lSTVq1dPR48e1cWLFyVJhw4dUsOGDd1VCgAAlYrb\nhs3btm2roKAgRUdHy+FwKCEhQampqfLz81Pnzp01ZMgQxcXFydPTU23atNHDDz/srlIAAKhU3HrO\ne9y4ccWmf3lOOzo6WtHR0e58ewAAKiXusAYAgGEIbwAADEN4AwBgGMIbAADDEN4AABiG8AYAwDCE\nNwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBjCGwAAw7j1wSQAcDsaPHtbRZfgVq9PCK3oEm57HHkD\nAGAYwhsAAMMQ3gAAGIbwBgDAMIQ3AACGIbwBADAM4Q0AgGH4nTdwA/jdLoBbCUfeAAAYhvAGAMAw\nhDcAAIYhvAEAMAzhDQCAYQhvAAAMQ3gDAGAYwhsAAMMQ3gAAGIbwBgDAMIQ3AACGIbwBADAM4Q0A\ngGEIbwAADEN4AwBgGMIbAADDEN4AABiG8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBjC\nGwAAwxDeAAAYhvAGAMAwhDcAAIbxcmfjM2fO1BdffCGHw6H4+Hi1atXKnvfzzz9r7NixKiwsVIsW\nLTR9+nR3lgIAQKXhtiPvvXv3Ki0tTevWrVNiYqISExOLzZ89e7YGDx6s9evXy9PTUydOnHBXKQAA\nVCpuC+/du3crLCxMktS4cWPl5uYqPz9fknTlyhUdOHBAoaGhkqSEhATdfffd7ioFAIBKxW3hnZWV\nJX9/f3s6ICBAmZmZkqTs7GxVr15ds2bNUr9+/TRv3jx3lQEAQKXj1nPev2RZVrG/T506pbi4ONWr\nV0/Dhg3TRx99pE6dOpW6vr9/NXl5eZZDpZVDYKBfRZdwS6JfXKNfXKNfXDOpX0yq9bdwW3g7nU5l\nZWXZ0xkZGQoMDJQk+fv76+6779Y999wjSWrfvr2+/fbb64Z3Ts55d5VaKWVm5lV0Cbck+sU1+sU1\n+sU1U/olMNDPmFpLU9qXD7cNm3fo0EGbN2+WJB0+fFhOp1O+vr6SJC8vLzVo0EA//vijPb9Ro0bu\nKgUAgErFbUfebdu2VVBQkKKjo+VwOJSQkKDU1FT5+fmpc+fOio+P14QJE2RZlpo0aWJfvAYAAK7P\nree8x40bV2y6WbNm9t/33nuv1qxZ4863BwCgUuIOawAAGIbwBgDAMIQ3AACGIbwBADAM4Q0AgGEI\nbwAADEN4AwBgGMIbAADDEN4AABiG8AYAwDBlhndubq7mzJlj3+p027Ztys7OdnthAADAtTLDe/Lk\nyapbt67S09MlSQUFBRo/frzbCwMAAK6VGd7Z2dmKi4uTt7e3JCkiIkIXL150e2EAAMC1GzrnXVhY\nKIfDIUnKysrS+fPn3VoUAAAoXZmPBI2JiVFUVJQyMzM1YsQIHTx4UJMmTSqP2gAAgAtlhne3bt3U\ntm1bffbZZ/Lx8dH06dPldDrLozYAAOBCmeE9evRoLVy4UF27di2PegAAQBnKDO/69etr/fr1atOm\njXx8fOzXGzRo4NbCAACAa2WG93vvvVfiNYfDoQ8++MAtBQEAgOsrM7y3bdtWHnUAAIAbVGZ4Z2Rk\naOHChTp48KAcDodat26t0aNHKyAgoDzqAwAAv1Lm77ynTp2qoKAgzZ8/X3PnztV9992n+Pj48qgN\nAAC4UOaR94ULFxQTE2NPN2nShKF0AAAqUJlH3hcuXFBGRoY9ffLkSRUUFLi1KAAAULoyj7yfe+45\n9erVS4GBgbIsS9nZ2UpMTCyP2gAAgAtlhnenTp20detW/fjjj5KkRo0aqUqVKu6uCwAAlKLMYfP9\n+/crISFBzZo1U7NmzTRixAjt27evPGoDAAAulBne8+bN03PPPWdPv/jii5o/f75biwIAAKUrM7wt\ny9K9995rT9evX18eHjf0JFEAAOAGZZ7zvvvuu/Xyyy+rXbt2sixL27dvV506dcqjNgAA4EKZh9Cz\nZs1S9erVtWbNGq1du1a1a9fWjBkzyqM2AADgQplH3lWqVFFcXJx8fX2VmZmptLQ0rjYHAKAClXnk\n/eKLL2rjxo06c+aM+vfvr5SUFE2bNq0cSgMAAK6UGd5fffWV+vTpo40bN6pnz55auHCh0tLSyqM2\nAADgwg1dbS5JH330kUJDQyWJ26MCAFCBygzvRo0a6cknn9S5c+fUvHlzbdiwQTVq1CiP2gAAgAtl\nXrA2Y8YMffPNN2rcuLEk6f7779dLL73k9sIAAIBrZYa3p6enmjdvbk+3bNnSrQUBAIDr41ZpAAAY\n5oYvWAMAALeGMsM7Li6uPOoAAAA3qMxz3s2bN9eiRYvUpk0beXt726+3b9/erYUBAADXygzv//3f\n/5V09bne1zgcDsIbAIAKUmZ4Jycnl3ht8+bNbikGAACUrczwPnHihFJSUpSTkyPp6t3V9uzZo/Dw\ncLcXBwAASirzgrX/+q//0l133aXPP/9cLVu2VE5ODjdpAQCgApUZ3p6enho2bJhq1aqlmJgYLV++\nXKtXry6P2gAAgAtlhvelS5d08uRJORwOHTt2TF5eXjp+/Hh51AYAAFwo85z3M888o927d2vIkCGK\njIyUp6enunfvXh61AQAAF0oN71OnTql27doKCwuzX9u7d6/OnTt3w08Vmzlzpr744gs5HA7Fx8er\nVatWJZaZN2+ePv/8c5dXtQMAgJJKHTbv0aOHhg0bpi1btqioqEiS5OXldcPBvXfvXqWlpWndunVK\nTExUYmJiiWW+++477du3718sHQCA21Op4b19+3Y99dRT+sc//qFOnTppzpw5Onr06A03vHv3bvuo\nvXHjxsrNzVV+fn6xZWbPnq0xY8b8i6UDAHB7KjW8q1Spou7du+vVV19VamqqatWqpTFjxig6Olrr\n168vs+GsrCz5+/vb0wEBAcrMzLSnU1NT1a5dO9WrV+8mNwEAgNtLmResSZLT6dSQIUPUqVMnLVu2\nTNOnT1dUVNRveqNfPp3szJkzSk1N1d/+9jedOnXqhtb3968mLy/P3/Set7PAQL+KLuGWRL+4Rr+4\nRr+4ZlK/mFTrb1FmeOfm5uqdd97RW2+9pYKCAkVFRWny5MllNux0OpWVlWVPZ2RkKDAwUJL06aef\nKjs7WzExMSooKNBPP/2kmTNnKj4+vtT2cnLO38j24P/LzMyr6BJuSfSLa/SLa/SLa6b0S2CgnzG1\nlqa0Lx+lhve2bdv01ltv6cCBA+rcubOmTp3q8mrx0nTo0EFJSUmKjo7W4cOH5XQ65evrK0mKiIhQ\nRESEJCk9PV0TJ068bnADAID/U2p4v/7664qKitLLL7+sO+644zc33LZtWwUFBSk6OloOh0MJCQlK\nTU2Vn5+fOnfufFNFAwBwOys1vFNSUm668XHjxhWbbtasWYll6tevz2+8AQD4Dcq8PSoAALi1EN4A\nABiG8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBjCGwAAwxDeAAAYhvAGAMAwhDcAAIYh\nvAEAMAzhDQCAYQhvAAAMQ3gDAGAYwhsAAMMQ3gAAGIbwBgDAMIQ3AACGIbwBADAM4Q0AgGEIbwAA\nDEN4AwBgGMIbAADDEN4AABiG8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBjCGwAAwxDe\nAAAYhvAGAMAwhDcAAIYhvAEAMAzhDQCAYQhvAAAMQ3gDAGAYwhsAAMMQ3gAAGIbwBgDAMIQ3AACG\nIbwBADAM4Q0AgGEIbwAADEN4AwBgGC93Nj5z5kx98cUXcjgcio+PV6tWrex5n376qebPny8PDw81\natRIiYmJ8vDguwQAAGVxW1ru3btXaWlpWrdunRITE5WYmFhs/tSpU7V48WKtXbtW586d0/bt291V\nCgAAlYrbwnv37t0KCwuTJDVu3Fi5ubnKz8+356empqpOnTqSpICAAOXk5LirFAAAKhW3hXdWVpb8\n/f3t6YCAAGVmZtrTvr6+kqSMjAzt3LlTISEh7ioFAIBKxa3nvH/JsqwSr50+fVojRoxQQkJCsaB3\nxd+/mry8PN1VXqUTGOhX0SXckugX1+gX1+gX10zqF5Nq/S3cFt5Op1NZWVn2dEZGhgIDA+3p/Px8\nDR06VKNHj1ZwcHCZ7eXknHdLnZVVZmZeRZdwS6JfXKNfXKNfXDOlXwID/YyptTSlfflw27B5hw4d\ntHnzZknS4cOH5XQ67aFySZo9e7YGDhyojh07uqsEAAAqJbcdebdt21ZBQUGKjo6Ww+FQQkKCUlNT\n5efnp+DgYG3YsEFpaWlav369JKl79+7q27evu8oBAKDScOs573HjxhWbbtasmf33oUOH3PnWAABU\nWtwVBQAAwxDeAAAYhvAGAMAwhDcAAIYhvAEAMAzhDQCAYQhvAAAMQ3gDAGAYwhsAAMMQ3gAAGIbw\nBgDAMIQ3AACGIbwBADAM4Q0AgGEIbwAADEN4AwBgGMIbAADDEN4AABiG8AYAwDCENwAAhiG8AQAw\nDOENAIBhCG8AAAxDeAMAYBjCGwAAwxDeAAAYhvAGAMAwhDcAAIYhvAEAMAzhDQCAYQhvAAAMQ3gD\nAGAYwhsAAMMQ3gAAGIbwBgDAMIQ3AACGIbwBADAM4Q0AgGEIbwAADEN4AwBgGMIbAADDEN4AABiG\n8AYAwDCENwAAhiG8AQAwDOENAIBhCG8AAAxDeAMAYBivii4AAHB7GDx7W0WX4FavTwgtt/dy65H3\nzJkz1bdvX0VHR+vLL78sNm/Xrl2KiopS3759tXTpUneWAQBApeK28N67d6/S0tK0bt06JSYmKjEx\nsdj8GTNmKCkpSWvWrNHOnTv13XffuasUAAAqFbeF9+7duxUWFiZJaty4sXJzc5Wfny9JOnbsmGrU\nqKG6devKw8NDISEh2r17t7tKAQCgUnFbeGdlZcnf39+eDggIUGZmpiQpMzNTAQEBLucBAIDrK7cL\n1izLuqn1AwP9fqdKrnp7XuTv2l5lQb+4Rr+4Rr+4Rr+4Rr/8ftx25O10OpWVlWVPZ2RkKDAw0OW8\nU6dOyel0uqsUAAAqFbeFd4cOHbR582ZJ0uHDh+V0OuXr6ytJql+/vvLz85Wenq6ioiJ9+OGH6tCh\ng7tKAQCgUnFYNzuefR1z587V/v375XA4lJCQoK+++kp+fn7q3Lmz9u3bp7lz50qSunTpoiFDhrir\nDAAAKhW3hjcAAPj9cXtUAAAMQ3gDAGAYwvt3kJ6erl69etnTW7duVUxMjNauXauQkBBdunTJnjdh\nwgSlp6crPT1dzZs319dff23PS01NVWpqarnWXppfb9M1iYmJOnbsmFvf++TJkxo6dKgGDBigqKgo\nTZw4UQUFBRo7dqx9EeQ1a9eu1YsvvqjU1FQ9/PDDKigosOfl5uaqZcuWbunT8tx/e/bs0ahRo0q8\n3rRpU23btq3YcklJSddt69f9d6vYtWuXYmNj7elTp04pPDxc+fn5+u///m9FRUWpf//+6tWrl1au\nXGkvFxsbq969eys2NlZRUVFlbn9l4OqzmZSUpC5duig2Ntb+3Lz//vsVVGHFSE9PV5s2bRQbG6vY\n2Fj17dtXU6ZM0eXLlxUaGqr+/fvb8375b81UhPfv7MiRI1q8eLGSkpLk4+OjO++8U2+88YbLZe+/\n/37NmzevnCu8OZMmTVKDBg3c+h6LFi1Sr169lJKSovXr18vb21vbt29X9+7dtXHjxmLLbty4Ud27\nd5ck3XXXXfr444/teVu2bFGdOnXcVmdF77+GDRtqyZIlunz58g0tn56ernfffdfNVf1rHnvsMdWt\nW1cbNmyQJM2ePVtjxozRkSNHtGbNGq1cuVJ///vftWrVKr377rvasWOHve6sWbOUnJysdevW6Z13\n3lFGRkZFbUaFiouLU3JyslJSUvTqq68qMTFRFy9erOiyylWjRo2UnJxs/3soLCzU22+/LUl65ZVX\n7HnJyckVXOnNI7x/R9nZ2Ro/frwWLFhg30Guf//+evvtt3XmzJkSywcFBalatWpG3Ro2NjZW33zz\njZKSkjRz5kwNHTpU4eHhdmhu2bJF0dHRGjBggGbPni1Jys/P1/DhwxUbG6s+ffrYD6np0qWLZsyY\noeXLlxd7j7Nnz9q30pWk6dOn64knntDjjz+uzz//XBcuXJAknT59WidPnlSbNm0kSSEhIfYHVboa\n7I899pjb+uJ6+2/16tWKjo5W//799frrr0u6enSUkpIiSfrmm2/sb/9dunTR6NGj9eabb2rXrl3q\n27evBgwYoOeee67YSMKvOZ1OPfroo3rrrbdKzHO1H6ZPn669e/dqyZIlN73t7jBhwgStWLFCW7du\n1blz5xQREaGUlBQ9//zz9s9MfX199fe//13BwcEl1j937pw8PT1VrVq18i79lnPXXXcpMDDwtr9z\nZatWrZSWllbRZbgF4f07KSoq0qhRo9S1a1c1btzYfr1KlSoaNGiQ/vKXv7hcb8yYMVq4cOFN34Gu\nIpw8eVKvvPKKJk2apHXr1uncuXNavny5Vq1apZSUFP388886cOCAMjMz1adPHyUnJ2vs2LF65ZVX\nJF3ts44dO+rZZ58t1u7QoUO1YMEC9evXT0uWLLE/fN7e3urYsaM+/PBDSVeHgCMiIuz1goKCdPTo\nUeXn5ysrK0uFhYX2jYHcxdX+O3bsmDZt2qQ1a9Zo9erV2rJli06cOFFqG8eOHdPIkSPVp08f5ebm\nau7cuUpJSZGvr2+xI0xXhg8frjfeeKPYEVZp+2HIkCFq166d/vSnP938hrtBQECABg0apNGjR2vK\nlCmSpO+//15NmjQptpy3t3ex6YkTJyo2NlYRERHq3bu3HfS3s++//16nT59W7dq1K7qUClNYWKgP\nPvhAQUFBFV2KW/A879/JDz8SeBaHAAAIAklEQVT8oAkTJuiNN95QZGRkseHanj17qk+fPjp+/HiJ\n9Ro2bKgWLVrovffeK89yfxdt27aVJNWpU0d5eXn67rvvdOLECfs3+3l5eTpx4oSaNGmiZcuW6bXX\nXlNBQUGxI6NWrVqVaLd169b64IMPtHPnTn3yySeKiorSggULFBwcrO7duys5OVndunXTpk2b7P/J\nXxMSEqKtW7cqPz9fTzzxhPLy8tzYA67338GDB5WWlqa4uDhJV8PU1b6/pmrVqnrggQckXQ2wyZMn\n6/Llyzp27JgeffRRVa9evdR1a9SoocjISK1atUoPPfSQJJW6H0y4i+GRI0dUr149HTp0SA0aNJCH\nh4d9WuCzzz7T/PnzdenSJbVo0ULTpk2TdHXYvEmTJiooKNCf/vQnNW/e3K0jLreqVatWafPmzcrP\nz1dBQYHmzp0rHx+fii6rXP3www/2iNaRI0f0zDPPKCwszB4l9PT0lCT5+/tr8eLFFVnqTSO8fycP\nPPCAYmJiVLNmTY0bN67YeW4PDw89//zzWrRokTw8Sg52jBw5UkOGDFFMTIy8vMzZJb+u1dvbWy1b\nttRrr71W7PUlS5aodu3aevnll3Xw4EG99NJLxdb5tYsXL6pq1aoKCwtTWFiY2rRpo3fffVfBwcH6\nt3/7N02ZMkXHjh1TXl6eHXrXREREaNmyZTp37pxeeuklrV+//nfcYtd+vf+8vb3VqVMnTZ8+vdhy\ne/bssf8uKiqy//5lH8THx2vFihVq3LhxifVLc+1irYYNG9rtudoPv3z/W9GXX36pb7/9VqtWrdKg\nQYPUsWNH3X///Tp48KDq1KmjNm3aKDk5WXv27NHq1atLrO/j46OQkBDt37//tgzvuLg4DRgwQBkZ\nGRo4cKCaNm1a0SWVu2vnvCVp1KhRatSokT3vlVdeue4XYdMwbP47i4iIUIMGDbR06dJir3fq1Ekn\nT57UkSNHSqxTq1YthYWFae3ateVVpls0atRIR48e1enTpyVJixcv1qlTp5STk6N77rlH0tUr8QsL\nC0tt48qVK+rRo0ex57ufPHlS9evXlyQ5HA6FhoZqzpw56tq1a4n1W7VqpePHj6uoqEh169b9PTev\nVL/ef0FBQdqzZ48uXLggy7I0Y8YMXbx4Ub6+vvY5yAMHDrhsKz8/X3Xr1tXZs2e1Z8+e6/bVNb8+\nNVPafvDw8Cj2peFWUlRUpGnTpmny5MmqXbu2evfuraSkJMXFxWnx4sX2tly5ckWffvppqUeUX375\nZbH/Yd+OnE6nevbsecte21Be/vznP2vu3Ln2NTKVjTmHeQaZPHmyevfurWHDhhV7fdy4cerTp4/L\ndQYPHqw1a9aUR3k37JdDUNLVD8P1VK1aVfHx8Ro6dKh8fHzUokULOZ1ORUZGavz48dq0aZNiYmL0\nzjvv6J///KfLNjw8PDRv3jx7SFS6ei/8qVOn2tM9evRQr169FB8f77KN4OBg1axZ8zds6c375f67\n++67FRcXp5iYGHl6eiosLEx33HGHOnfurOHDh+vLL7/Uww8/7LKd/v37q1+/fmrYsKGeeeYZJSUl\naezYsWW+f8+ePfW3v/1NUun7wdvbW1999ZVmzpxZat9VlNdff13t2rWzR1Li4uLUq1cvPf300xo/\nfryGDx8ub29vXbp0Sa1bty52umTixImqVq2aCgsL1bRpUz355JMVtRnl5tefzTvuuEMhISH29KBB\ng+zPya9Hp24XDRo0UHh4eIkLYisLbo8KAIBhGDYHAMAwhDcAAIYhvAEAMAzhDQCAYQhvAAAMQ3gD\nlVx6erqaNm1a4qeI+/fvV9OmTW/45i1vvvmmJkyYcN1lYmNjtWvXrn+5VgA3hvAGbgMNGzYs8bjS\n1NTU2/6GJoCpuEkLcBtwOp26dOmSvv32Wz3wwAO6cOGCDhw4YN8Pff369Vq7dq2qVq2qmjVrasaM\nGfL19dXq1au1Zs0a1alTp9i90b/++mvNmTNHRUVFKiws1NSpU9WiRQt7/qlTpzRu3DhJV29327dv\nX0VFRZXvRgOVGEfewG0iMjLSvrPd5s2b1bFjR3l4eOjnn39WUlKSVq5cqeTkZNWtW1crV65UXl6e\nFi9erOTkZL366qvKycmx2/rzn/+sF154QcnJyfZtTX9p48aNuu++++znS99uz5UG3I3wBm4TXbt2\n1caNG1VUVKS33npLTz31lKSrz8gOCgqyH6XZrl07+8lo9erVk7+/vyTpkUcekXT1Oeo//PCDJk2a\npNjYWCUmJio/P19Xrlyx3+vxxx/X7t27NWHCBG3btk19+/Yt560FKjeGzYHbREBAgFq0aKH169cr\nMzNTDz74oMvlLMuSw+Gw/3vNtXD28fGRt7e3/fQmVxo3bqx3331X+/bt06ZNm/TGG28Y/+Ad4FbC\nkTdwG4mMjNSCBQuKPbzj3LlzOnz4sPLz8yVJu3bt0kMPPaR77rlH6enpOnv2rCzL0u7duyVJfn5+\nql+/vj7++GNJVx+S8esnWL399ts6ePCgHnvsMSUkJOjnn3++ZZ9oBpiII2/gNhIaGqqpU6faQ+aS\nVKdOHf3Hf/yHBg0aJB8fH9WpU0djx45VtWrVNGLECMXExKhevXqqV6+efe56zpw5mjFjhlasWKGi\noqISPyG7//77lZCQIB8fH1mWpaFDhxr1rHrgVsdTxQAAMAzD5gAAGIbwBgDAMIQ3AACGIbwBADAM\n4Q0AgGEIbwAADEN4AwBgGMIbAADD/D8OABDWZtM8bgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFMCAYAAADiATSNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nwf7h-U132hh",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "fi = imodel.fit(X, y).feature_importances_\n",
        "pos = np.arange(len(X.columns))\n",
        "print(\"Best performing model\", imodel)\n",
        "plt.figure(figsize=(13, 8))\n",
        "plt.barh(pos, fi)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.xlabel(\"Model Accuracy\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.yticks(pos, (list(X)))\n",
        "plt.show()"
      ]
    },
    {
      "metadata": {
        "id": "iLM5RLn932hi",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "preds = imodel.predict(X_test)\n",
        "print('Variance score: %.4f' % r2_score(test.label, preds))\n",
        "\n",
        "pd.DataFrame({\"ImageId\": list(range(1,len(preds)+1)), \"Label\": preds, \"test_label\":test.label})\n"
      ]
    },
    {
      "metadata": {
        "id": "8_BWyVaJ32hi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}